{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tokenizer import porter_stem_tokenizer\n",
    "from tokenizer import snowball_stem_tokenizer\n",
    "from tokenizer import lancaster_stem_tokenizer\n",
    "from tokenizer import wordnet_lemma_tokenizer\n",
    "from tokenizer import wordnet_lemma_pos_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology</td>\n",
       "      <td>21st-Century Sports: How Digital Technology Is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Asian quake hits European shares Shares in Eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "      <td>BT offers free net phone calls BT is offering ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Barclays shares up on merger talk Shares in UK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sport</td>\n",
       "      <td>Barkley fit for match in Ireland England centr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>sport</td>\n",
       "      <td>Woodward eyes Brennan for Lions Toulouse's for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom trial starts in New York The trial of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos accused of lying to court Russian oil fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos drops banks from court bid Russian oil c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>sport</td>\n",
       "      <td>Zambia confident and cautious Zambia's technic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1408 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                            content\n",
       "0     technology  21st-Century Sports: How Digital Technology Is...\n",
       "1       business  Asian quake hits European shares Shares in Eur...\n",
       "2     technology  BT offers free net phone calls BT is offering ...\n",
       "3       business  Barclays shares up on merger talk Shares in UK...\n",
       "4          sport  Barkley fit for match in Ireland England centr...\n",
       "...          ...                                                ...\n",
       "1403       sport  Woodward eyes Brennan for Lions Toulouse's for...\n",
       "1404    business  WorldCom trial starts in New York The trial of...\n",
       "1405    business  Yukos accused of lying to court Russian oil fi...\n",
       "1406    business  Yukos drops banks from court bid Russian oil c...\n",
       "1407       sport  Zambia confident and cautious Zambia's technic...\n",
       "\n",
       "[1408 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_PATH = os.getcwd()\n",
    "\n",
    "with open(f'{ROOT_PATH}/data/datastore/article_titles_plus_contents_all.txt', mode='r', encoding='utf-8') as file:\n",
    "  raw_contents = file.read().splitlines()\n",
    "\n",
    "with open(f'{ROOT_PATH}/data/target/article_categories_all.txt', mode='r', encoding='utf-8') as file:\n",
    "  target = file.read().splitlines()\n",
    "\n",
    "raw_df = pd.DataFrame({\n",
    "    'category': target,\n",
    "    'content': raw_contents\n",
    "})\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tokenizing(tokenizer, X):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenizer, min_df=.01)\n",
    "    tf_idf = vectorizer.fit_transform(X)\n",
    "    return tf_idf\n",
    "\n",
    "def model_testing(tokenizers, models, X, Y):\n",
    "    cv_result = {\n",
    "        \"model_name\": [],\n",
    "        \"tokenizer_name\": [],\n",
    "        \"score (default_params)\": []\n",
    "    }\n",
    "    for tokenizer in tokenizers:\n",
    "        tokenized = tokenizing(tokenizer, X)\n",
    "        for model in models:\n",
    "            cv_score = cross_val_score(model, tokenized, Y, cv=5, )\n",
    "            cv_result[\"model_name\"].append(type(model).__name__)\n",
    "            cv_result[\"tokenizer_name\"].append(tokenizer.__name__)\n",
    "            cv_result[\"score (default_params)\"].append(cv_score.mean())\n",
    "    result_df = pd.DataFrame(cv_result).sort_values(by=['score (default_params)'], ascending=False)\n",
    "    return result_df\n",
    "\n",
    "def parameter_tuning(tokenizers, models, params, X, Y):\n",
    "    tuning_result = {\n",
    "        \"model_name\": [],\n",
    "        \"tokenizer_name\": [],\n",
    "        \"best_parameter\": [],\n",
    "        \"best_score\": []\n",
    "    }\n",
    "    for tokenizer in tokenizers:\n",
    "        tokenized = tokenizing(tokenizer, X)\n",
    "        for model, param in zip(models, params.values()):\n",
    "            clf = GridSearchCV(model, param, cv=5, n_jobs=-1, verbose=1)\n",
    "            result = clf.fit(tokenized, Y)\n",
    "            tuning_result['model_name'].append(type(model).__name__)\n",
    "            tuning_result['tokenizer_name'].append(tokenizer.__name__)\n",
    "            tuning_result['best_score'].append(result.best_score_)\n",
    "            tuning_result['best_parameter'].append(result.best_params_)\n",
    "    tuning_result = pd.DataFrame(tuning_result).sort_values(by=['best_score'], ascending=False)\n",
    "    return tuning_result\n",
    "\n",
    "def compare_result(untune_result, tuned_result):\n",
    "    compare_result = pd.merge(untune_result,\n",
    "                                tuned_result,\n",
    "                                how='inner',\n",
    "                                on=['model_name', 'tokenizer_name']\n",
    "                                )[['model_name', 'tokenizer_name', 'score (default_params)', 'best_score', 'best_parameter']]\n",
    "    compare_result['variance'] = compare_result['best_score'] - compare_result['score (default_params)']\n",
    "    return compare_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer\n",
    "\n",
    "# tokenizers = [porter_stem_tokenizer, snowball_stem_tokenizer,\n",
    "#               lancaster_stem_tokenizer, wordnet_lemma_tokenizer, \n",
    "#               wordnet_lemma_pos_tokenizer]\n",
    "\n",
    "tokenizers = [porter_stem_tokenizer, snowball_stem_tokenizer,]\n",
    "\n",
    "# Define model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "models = [KNeighborsClassifier(), RandomForestClassifier()]\n",
    "\n",
    "# Define parameter\n",
    "params ={\n",
    "    \"knn_params\":{\n",
    "        'n_neighbors' : list(range(1, 16))\n",
    "    },\n",
    "    \"r2f_params\":{\n",
    "        'max_depth': list(range(50, 60, 2)),\n",
    "        'min_samples_split': list(range(2, 10, 2))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_df['content'].values\n",
    "Y = raw_df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>tokenizer_name</th>\n",
       "      <th>score (default_params)</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_parameter</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.970869</td>\n",
       "      <td>0.975132</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 8}</td>\n",
       "      <td>0.004263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.969440</td>\n",
       "      <td>0.975132</td>\n",
       "      <td>{'max_depth': 50, 'min_samples_split': 6}</td>\n",
       "      <td>0.005691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.966616</td>\n",
       "      <td>0.967330</td>\n",
       "      <td>{'n_neighbors': 7}</td>\n",
       "      <td>0.000714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.965907</td>\n",
       "      <td>0.968751</td>\n",
       "      <td>{'n_neighbors': 7}</td>\n",
       "      <td>0.002844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name           tokenizer_name  score (default_params)  \\\n",
       "0  RandomForestClassifier    porter_stem_tokenizer                0.970869   \n",
       "1  RandomForestClassifier  snowball_stem_tokenizer                0.969440   \n",
       "2    KNeighborsClassifier    porter_stem_tokenizer                0.966616   \n",
       "3    KNeighborsClassifier  snowball_stem_tokenizer                0.965907   \n",
       "\n",
       "   best_score                             best_parameter  variance  \n",
       "0    0.975132  {'max_depth': 50, 'min_samples_split': 8}  0.004263  \n",
       "1    0.975132  {'max_depth': 50, 'min_samples_split': 6}  0.005691  \n",
       "2    0.967330                         {'n_neighbors': 7}  0.000714  \n",
       "3    0.968751                         {'n_neighbors': 7}  0.002844  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run pipline\n",
    "test_result = model_testing(tokenizers, models, X, Y)\n",
    "tuned_result = parameter_tuning(tokenizers, models, params, X, Y)\n",
    "compare_result(test_result, tuned_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d853cb24e8f7628e3ba29dc09518236c084a73039b3bc92092f3293b2fe13772"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
