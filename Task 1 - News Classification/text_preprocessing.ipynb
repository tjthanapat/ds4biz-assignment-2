{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{ROOT_PATH}/data/datastore/article_titles_plus_contents_all.txt', mode='r', encoding='utf-8') as file:\n",
    "  raw_contents = file.read().splitlines()\n",
    "\n",
    "with open(f'{ROOT_PATH}/data/target/article_categories_all.txt', mode='r', encoding='utf-8') as file:\n",
    "  target = file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology</td>\n",
       "      <td>21st-Century Sports: How Digital Technology Is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Asian quake hits European shares Shares in Eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "      <td>BT offers free net phone calls BT is offering ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Barclays shares up on merger talk Shares in UK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sport</td>\n",
       "      <td>Barkley fit for match in Ireland England centr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>sport</td>\n",
       "      <td>Woodward eyes Brennan for Lions Toulouse's for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom trial starts in New York The trial of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos accused of lying to court Russian oil fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos drops banks from court bid Russian oil c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>sport</td>\n",
       "      <td>Zambia confident and cautious Zambia's technic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                            content\n",
       "0     technology  21st-Century Sports: How Digital Technology Is...\n",
       "1       business  Asian quake hits European shares Shares in Eur...\n",
       "2     technology  BT offers free net phone calls BT is offering ...\n",
       "3       business  Barclays shares up on merger talk Shares in UK...\n",
       "4          sport  Barkley fit for match in Ireland England centr...\n",
       "...          ...                                                ...\n",
       "1403       sport  Woodward eyes Brennan for Lions Toulouse's for...\n",
       "1404    business  WorldCom trial starts in New York The trial of...\n",
       "1405    business  Yukos accused of lying to court Russian oil fi...\n",
       "1406    business  Yukos drops banks from court bid Russian oil c...\n",
       "1407       sport  Zambia confident and cautious Zambia's technic...\n",
       "\n",
       "[1408 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.DataFrame({\n",
    "    'category': target,\n",
    "    'content': raw_contents\n",
    "})\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "SKLEARN_STOP_WORDS = ENGLISH_STOP_WORDS\n",
    "\n",
    "# Merge stop words from ntlk and sklearn\n",
    "STOP_WORDS = NLTK_STOP_WORDS.union(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def word_tokenizer(text:str)->list:\n",
    "  \"\"\"\n",
    "  Tokenize given text using NLTK's word tokenizer\n",
    "  with normalizing (lowercasing string) and filtering \n",
    "  stop words, numbers and punctuation marks.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenize(text.lower())\n",
    "\n",
    "  tokens_to_return = list()\n",
    "  for token in tokens:\n",
    "    token = token.strip(\"'\")\n",
    "\n",
    "    # Filter number\n",
    "    if (re.match(r\"^[\\d.]+$\", token)): \n",
    "      continue\n",
    "    # Filter punctuation mark and stop word\n",
    "    elif (re.match(r\"[\\w'-]+\", token) and (token not in ['-',\"'\"]) \n",
    "    and (token not in STOP_WORDS)):\n",
    "      tokens_to_return.append(token)\n",
    "\n",
    "  return tokens_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "It's 21-century education. \n",
      "They are Mr. and Mrs. Brown. \n",
      "While this seems like a cliché, it is true. \n",
      "6.80 pounds :; or £6.80\n",
      "— em dash – en dash - hyphen\n",
      "----------\n",
      "NLTK's word tokenizer:\n",
      "['It', \"'s\", '21-century', 'education', '.', 'They', 'are', 'Mr.', 'and', 'Mrs.', 'Brown', '.', 'While', 'this', 'seems', 'like', 'a', 'cliché', ',', 'it', 'is', 'true', '.', '6.80', 'pounds', ':', ';', 'or', '£6.80', '—', 'em', 'dash', '–', 'en', 'dash', '-', 'hyphen']\n",
      "----------\n",
      "Custom word tokenizer:\n",
      "['21-century', 'education', 'mr.', 'mrs.', 'brown', 'like', 'cliché', 'true', 'pounds', 'em', 'dash', 'en', 'dash', 'hyphen']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"It's 21-century education. \n",
    "They are Mr. and Mrs. Brown. \n",
    "While this seems like a cliché, it is true. \n",
    "6.80 pounds :; or £6.80\n",
    "— em dash – en dash - hyphen\"\"\"\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print('-'*10)\n",
    "print(\"NLTK's word tokenizer:\")\n",
    "print(word_tokenize(test_text))\n",
    "print('-'*10)\n",
    "print(\"Custom word tokenizer:\")\n",
    "print(word_tokenizer(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer with Stemmer or Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer with Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option1: NLTK PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def porter_stem_tokenizer(text:str)->list:\n",
    "  \"\"\"\n",
    "  Tokenize given text using custom word tokenizer\n",
    "  (based on NLTK word tokenizer) with Porter stemmer\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenizer(text)\n",
    "  stemmer = PorterStemmer()\n",
    "  stems = list()\n",
    "  for token in tokens:\n",
    "    stems.append(stemmer.stem(token))\n",
    "  return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option2: NLTK SnowballStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer as SnowballStemmer\n",
    "\n",
    "def snowball_stem_tokenizer(text:str)->list:\n",
    "  \"\"\"\n",
    "  Tokenize given text using custom word tokenizer\n",
    "  (based on NLTK word tokenizer) with Snowball stemmer\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenizer(text)\n",
    "  stemmer = SnowballStemmer()\n",
    "  stems = list()\n",
    "  for token in tokens:\n",
    "      stems.append(stemmer.stem(token))\n",
    "  return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option3: NLTK LancasterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def lancaster_stem_tokenizer(text):\n",
    "  \"\"\"\n",
    "  Tokenize given text using custom word tokenizer\n",
    "  (based on NLTK word tokenizer) with Lancaster stemmer\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenizer(text)\n",
    "  stemmer = LancasterStemmer()\n",
    "  stems = list()\n",
    "  for token in tokens:\n",
    "    stems.append(stemmer.stem(token))\n",
    "  return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer with Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option4: NLTK WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def convert_tag(tag):\n",
    "  \"\"\"\n",
    "  Convert part-of-speech tag to tag compatible \n",
    "  with WordNet lemmatizer.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  tag : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  str\n",
    "    Part-of-speech tag compatible with WordNet lemmatizer; \n",
    "    \"n\" for noun, \"v\" for verb, \"a\" for adjective and \"r\" for adverb\n",
    "  \"\"\"\n",
    "  if tag[0] == 'V':\n",
    "    return 'v'\n",
    "  elif tag[0] == 'J':\n",
    "    return 'a'\n",
    "  elif tag[0] == 'R':\n",
    "    return 'r'\n",
    "  else:\n",
    "    return 'n'\n",
    "\n",
    "def wordnet_lemma_tokenizer(text, with_pos:bool=False):\n",
    "  \"\"\"\n",
    "  Tokenize given text using custom word tokenizer\n",
    "  (based on NLTK word tokenizer) with Wordnet Lemmatizer\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "  with_pos : bool, default False\n",
    "    Flag indicating lemmatizing words with or without pos.  \n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenizer(text)\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmas = list()\n",
    "  \n",
    "  if (with_pos):\n",
    "    tokens_with_pos_tag = pos_tag(tokens)\n",
    "    for token in tokens_with_pos_tag:\n",
    "      word = token[0]\n",
    "      pos = convert_tag(token[1])\n",
    "      lemmas.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "  else:\n",
    "    for token in tokens:\n",
    "      lemmas.append(lemmatizer.lemmatize(token))\n",
    "  return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def term_weighting(tokenizer, all_texts):\n",
    "  \"\"\"\n",
    "  Parameters\n",
    "  ----------\n",
    "  tokenizer : list\n",
    "    A function of stemmer or lemmatizer method\n",
    "\n",
    "  all_text : list\n",
    "    List of all contents\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  matrix\n",
    "    Weighting matrix\n",
    "  \"\"\"\n",
    "  vectorizer = TfidfVectorizer(tokenizer=tokenizer,\n",
    "                              min_df=0.01)\n",
    "  term_weighted = vectorizer.fit_transform(all_texts)\n",
    "  return term_weighted\n",
    "\n",
    "text_feature = term_weighting(snowball_stem_tokenizer, raw_df['contents'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 286)\t0.14244186559641372\n",
      "  (0, 394)\t0.08897947797666445\n",
      "  (0, 129)\t0.08345971352777776\n",
      "  (0, 569)\t0.0800371539108057\n",
      "  (0, 1783)\t0.08028163527649192\n",
      "  (0, 1963)\t0.08789475802964346\n",
      "  (0, 183)\t0.104781121759625\n",
      "  (0, 583)\t0.06419716487237341\n",
      "  (0, 1053)\t0.052560693555919115\n",
      "  (0, 1538)\t0.0720375362840792\n",
      "  (0, 2283)\t0.10727192703393339\n",
      "  (0, 550)\t0.09869739315309342\n",
      "  (0, 1137)\t0.09543750666594165\n",
      "  (0, 1385)\t0.1055792316696807\n",
      "  (0, 2179)\t0.07751935769141835\n",
      "  (0, 1138)\t0.06663058231608103\n",
      "  (0, 1671)\t0.06967664086823136\n",
      "  (0, 244)\t0.0858791952855784\n",
      "  (0, 765)\t0.10640864788093655\n",
      "  (0, 950)\t0.09152007037477075\n",
      "  (0, 2344)\t0.04267942348354652\n",
      "  (0, 1865)\t0.06550077880753861\n",
      "  (0, 2360)\t0.03208555019953715\n",
      "  (0, 1773)\t0.08403913066572191\n",
      "  (0, 625)\t0.06908917524204597\n",
      "  :\t:\n",
      "  (1407, 1845)\t0.08605860434921996\n",
      "  (1407, 988)\t0.05734745954919708\n",
      "  (1407, 1997)\t0.09245945876726355\n",
      "  (1407, 860)\t0.09305340136082642\n",
      "  (1407, 1437)\t0.09397275350262445\n",
      "  (1407, 1586)\t0.10714311935072668\n",
      "  (1407, 2111)\t0.0661320173826321\n",
      "  (1407, 1438)\t0.06266378159088326\n",
      "  (1407, 1360)\t0.07053712384957576\n",
      "  (1407, 1436)\t0.054826807230546955\n",
      "  (1407, 1084)\t0.0637948937370219\n",
      "  (1407, 2080)\t0.08768602846258274\n",
      "  (1407, 671)\t0.07408827939106394\n",
      "  (1407, 2288)\t0.054704397536917866\n",
      "  (1407, 667)\t0.08270662289266204\n",
      "  (1407, 575)\t0.06397373064177624\n",
      "  (1407, 1826)\t0.028177444257399598\n",
      "  (1407, 0)\t0.055991511082548386\n",
      "  (1407, 2290)\t0.0753508495257\n",
      "  (1407, 3)\t0.10425705864002562\n",
      "  (1407, 950)\t0.1561449703274743\n",
      "  (1407, 2360)\t0.036494707396470094\n",
      "  (1407, 981)\t0.06207605891362937\n",
      "  (1407, 849)\t0.08697503720551338\n",
      "  (1407, 2153)\t0.04470859542591288\n"
     ]
    }
   ],
   "source": [
    "print(text_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_feature\n",
    "Y = raw_df['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=7, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.94      0.97      0.96       158\n",
      "       sport       0.98      0.98      0.98       172\n",
      "  technology       0.98      0.93      0.95       135\n",
      "\n",
      "    accuracy                           0.96       465\n",
      "   macro avg       0.96      0.96      0.96       465\n",
      "weighted avg       0.96      0.96      0.96       465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(Y_test, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fe7d652243bcdb6cde2e69617dc664bf14361ea2de14b139d2fec55ec2ae7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
