{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{ROOT_PATH}/data/datastore/article_titles_plus_contents_all.txt', mode='r', encoding='utf-8') as file:\n",
    "   raw_contents = file.read().splitlines()\n",
    "\n",
    "with open(f'{ROOT_PATH}/data/target/article_categories_all.txt', mode='r', encoding='utf-8') as file:\n",
    "   target = file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology</td>\n",
       "      <td>21st-Century Sports: How Digital Technology Is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Asian quake hits European shares Shares in Eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "      <td>BT offers free net phone calls BT is offering ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Barclays shares up on merger talk Shares in UK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sport</td>\n",
       "      <td>Barkley fit for match in Ireland England centr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>sport</td>\n",
       "      <td>Woodward eyes Brennan for Lions Toulouse's for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom trial starts in New York The trial of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos accused of lying to court Russian oil fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos drops banks from court bid Russian oil c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>sport</td>\n",
       "      <td>Zambia confident and cautious Zambia's technic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1408 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      categories                                           contents\n",
       "0     technology  21st-Century Sports: How Digital Technology Is...\n",
       "1       business  Asian quake hits European shares Shares in Eur...\n",
       "2     technology  BT offers free net phone calls BT is offering ...\n",
       "3       business  Barclays shares up on merger talk Shares in UK...\n",
       "4          sport  Barkley fit for match in Ireland England centr...\n",
       "...          ...                                                ...\n",
       "1403       sport  Woodward eyes Brennan for Lions Toulouse's for...\n",
       "1404    business  WorldCom trial starts in New York The trial of...\n",
       "1405    business  Yukos accused of lying to court Russian oil fi...\n",
       "1406    business  Yukos drops banks from court bid Russian oil c...\n",
       "1407       sport  Zambia confident and cautious Zambia's technic...\n",
       "\n",
       "[1408 rows x 2 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.DataFrame({\n",
    "    'categories': target,\n",
    "    'contents': raw_contents\n",
    "})\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge stop words from ntlk and sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "stop_words = set(stopwords.words('english')).union(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# def sklearn_tokenizer(text):\n",
    "#     tokenizer = CountVectorizer(stop_words=stop_words).build_tokenizer()\n",
    "#     tokens = tokenizer(text)\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def nltk_word_tokenizer(text):\n",
    "  tokens = word_tokenize(text)\n",
    "  tokens_to_return = list()\n",
    "  for token in tokens:\n",
    "    if(re.match(r\"[\\w'-]+\", token) and (token not in ['-',\"'\"]) and (token not in stop_words)):\n",
    "      tokens_to_return.append(token)\n",
    "\n",
    "  return tokens_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option1: nltk PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def porter_stem_tokenizer(text):\n",
    "    tokens = nltk_word_tokenizer(text.lower())\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = list()\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option2: nltk SnowballStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer as SnowballStemmer\n",
    "\n",
    "def snowball_stem_tokenizer(text):\n",
    "    tokens = nltk_word_tokenizer(text.lower())\n",
    "    stemmer = SnowballStemmer()\n",
    "    stems = list()\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option3: nltk LancasterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def lancaster_stem_tokenizer(text):\n",
    "    tokens = nltk_word_tokenizer(text.lower())\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = list()\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer without pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option4: nltk WordNetLemmatizer without pos tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def wordnet_lemma_tokenizer(text):\n",
    "    tokens = nltk_word_tokenizer(text.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = list()\n",
    "    for token in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(token))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer with pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option5: nltk WordNetLemmatizer with pos tag\n",
    "def convert_tag(tag):\n",
    "  if tag[0] == 'V':\n",
    "    return 'v'\n",
    "  elif tag[0] == 'J':\n",
    "    return 'a'\n",
    "  elif tag[0] == 'R':\n",
    "    return 'r'\n",
    "  else:\n",
    "    return 'n'\n",
    "\n",
    "def wordnet_lemma_pos_tokenizer(text):\n",
    "  tokens = nltk_word_tokenizer(text.lower())\n",
    "  tokens_with_pos_tag = nltk.pos_tag(tokens)\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemma_tokens = list()\n",
    "  for token in tokens_with_pos_tag:\n",
    "    word = token[0]\n",
    "    pos = convert_tag(token[1])\n",
    "    lemma_tokens.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "  return lemma_tokens  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def term_weighting(tokenizer, all_texts):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : list\n",
    "        A function of stemmer or lemmatizer method\n",
    "\n",
    "    all_text : list\n",
    "        List of all contents\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    matrix\n",
    "        Weighting matrix\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenizer,\n",
    "                                min_df=0.01)\n",
    "    term_weighted = vectorizer.fit_transform(all_texts)\n",
    "    return term_weighted\n",
    "\n",
    "text_feature = term_weighting(snowball_stem_tokenizer, raw_df['contents'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat text feature to dataframe\n",
    "raw_df['weight_feature'] = text_feature.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>contents</th>\n",
       "      <th>weight_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology</td>\n",
       "      <td>21st-Century Sports: How Digital Technology Is...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Asian quake hits European shares Shares in Eur...</td>\n",
       "      <td>[0.05315856074546948, 0.0, 0.0, 0.173218607356...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "      <td>BT offers free net phone calls BT is offering ...</td>\n",
       "      <td>[0.03556298211753117, 0.0, 0.0, 0.011036470351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Barclays shares up on merger talk Shares in UK...</td>\n",
       "      <td>[0.022094298001699646, 0.0, 0.0, 0.02056996210...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sport</td>\n",
       "      <td>Barkley fit for match in Ireland England centr...</td>\n",
       "      <td>[0.03651482394555501, 0.0668501490153514, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>sport</td>\n",
       "      <td>Woodward eyes Brennan for Lions Toulouse's for...</td>\n",
       "      <td>[0.10142723315034727, 0.0, 0.0, 0.118036921912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom trial starts in New York The trial of...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos accused of lying to court Russian oil fi...</td>\n",
       "      <td>[0.01063187804449671, 0.0, 0.0, 0.039593442338...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos drops banks from court bid Russian oil c...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>sport</td>\n",
       "      <td>Zambia confident and cautious Zambia's technic...</td>\n",
       "      <td>[0.055991511082548386, 0.0, 0.0, 0.10425705864...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1408 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      categories                                           contents  \\\n",
       "0     technology  21st-Century Sports: How Digital Technology Is...   \n",
       "1       business  Asian quake hits European shares Shares in Eur...   \n",
       "2     technology  BT offers free net phone calls BT is offering ...   \n",
       "3       business  Barclays shares up on merger talk Shares in UK...   \n",
       "4          sport  Barkley fit for match in Ireland England centr...   \n",
       "...          ...                                                ...   \n",
       "1403       sport  Woodward eyes Brennan for Lions Toulouse's for...   \n",
       "1404    business  WorldCom trial starts in New York The trial of...   \n",
       "1405    business  Yukos accused of lying to court Russian oil fi...   \n",
       "1406    business  Yukos drops banks from court bid Russian oil c...   \n",
       "1407       sport  Zambia confident and cautious Zambia's technic...   \n",
       "\n",
       "                                         weight_feature  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1     [0.05315856074546948, 0.0, 0.0, 0.173218607356...  \n",
       "2     [0.03556298211753117, 0.0, 0.0, 0.011036470351...  \n",
       "3     [0.022094298001699646, 0.0, 0.0, 0.02056996210...  \n",
       "4     [0.03651482394555501, 0.0668501490153514, 0.0,...  \n",
       "...                                                 ...  \n",
       "1403  [0.10142723315034727, 0.0, 0.0, 0.118036921912...  \n",
       "1404  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1405  [0.01063187804449671, 0.0, 0.0, 0.039593442338...  \n",
       "1406  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1407  [0.055991511082548386, 0.0, 0.0, 0.10425705864...  \n",
       "\n",
       "[1408 rows x 3 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d853cb24e8f7628e3ba29dc09518236c084a73039b3bc92092f3293b2fe13772"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
