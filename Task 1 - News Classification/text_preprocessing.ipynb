{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{ROOT_PATH}/data/datastore/article_titles_plus_contents_all.txt', mode='r', encoding='utf-8') as file:\n",
    "  raw_contents = file.read().splitlines()\n",
    "\n",
    "with open(f'{ROOT_PATH}/data/target/article_categories_all.txt', mode='r', encoding='utf-8') as file:\n",
    "  target = file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology</td>\n",
       "      <td>21st-Century Sports: How Digital Technology Is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Asian quake hits European shares Shares in Eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "      <td>BT offers free net phone calls BT is offering ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Barclays shares up on merger talk Shares in UK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sport</td>\n",
       "      <td>Barkley fit for match in Ireland England centr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>sport</td>\n",
       "      <td>Woodward eyes Brennan for Lions Toulouse's for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom trial starts in New York The trial of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos accused of lying to court Russian oil fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos drops banks from court bid Russian oil c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>sport</td>\n",
       "      <td>Zambia confident and cautious Zambia's technic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                            content\n",
       "0     technology  21st-Century Sports: How Digital Technology Is...\n",
       "1       business  Asian quake hits European shares Shares in Eur...\n",
       "2     technology  BT offers free net phone calls BT is offering ...\n",
       "3       business  Barclays shares up on merger talk Shares in UK...\n",
       "4          sport  Barkley fit for match in Ireland England centr...\n",
       "...          ...                                                ...\n",
       "1403       sport  Woodward eyes Brennan for Lions Toulouse's for...\n",
       "1404    business  WorldCom trial starts in New York The trial of...\n",
       "1405    business  Yukos accused of lying to court Russian oil fi...\n",
       "1406    business  Yukos drops banks from court bid Russian oil c...\n",
       "1407       sport  Zambia confident and cautious Zambia's technic...\n",
       "\n",
       "[1408 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.DataFrame({\n",
    "    'category': target,\n",
    "    'content': raw_contents\n",
    "})\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "SKLEARN_STOP_WORDS = ENGLISH_STOP_WORDS\n",
    "\n",
    "# Merge stop words from ntlk and sklearn\n",
    "STOP_WORDS = NLTK_STOP_WORDS.union(SKLEARN_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def word_tokenizer(text:str)->list:\n",
    "  \"\"\"\n",
    "  Tokenize given text using NLTK's word tokenizer\n",
    "  with normalizing (lowercasing string) and filtering \n",
    "  stop words, numbers and punctuation marks.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenize(text.lower())\n",
    "\n",
    "  tokens_to_return = list()\n",
    "  for token in tokens:\n",
    "    token = token.strip(\"'\")\n",
    "\n",
    "    # Filter number\n",
    "    if (re.match(r\"^[\\d.]+$\", token)): \n",
    "      continue\n",
    "    # Filter punctuation mark and stop word\n",
    "    elif (re.match(r\"[\\w'-]+\", token) and (token not in ['-',\"'\"]) \n",
    "    and (token not in STOP_WORDS)):\n",
    "      tokens_to_return.append(token)\n",
    "\n",
    "  return tokens_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "It's 21-century education. \n",
      "They are Mr. and Mrs. Brown. \n",
      "While this seems like a cliché, it is true. \n",
      "6.80 pounds :; or £6.80\n",
      "— em dash – en dash - hyphen\n",
      "----------\n",
      "NLTK's word tokenizer:\n",
      "['It', \"'s\", '21-century', 'education', '.', 'They', 'are', 'Mr.', 'and', 'Mrs.', 'Brown', '.', 'While', 'this', 'seems', 'like', 'a', 'cliché', ',', 'it', 'is', 'true', '.', '6.80', 'pounds', ':', ';', 'or', '£6.80', '—', 'em', 'dash', '–', 'en', 'dash', '-', 'hyphen']\n",
      "----------\n",
      "Custom word tokenizer:\n",
      "['21-century', 'education', 'mr.', 'mrs.', 'brown', 'like', 'cliché', 'true', 'pounds', 'em', 'dash', 'en', 'dash', 'hyphen']\n"
     ]
    }
   ],
   "source": [
    "test_text = \"\"\"It's 21-century education. \n",
    "They are Mr. and Mrs. Brown. \n",
    "While this seems like a cliché, it is true. \n",
    "6.80 pounds :; or £6.80\n",
    "— em dash – en dash - hyphen\"\"\"\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print('-'*10)\n",
    "print(\"NLTK's word tokenizer:\")\n",
    "print(word_tokenize(test_text))\n",
    "print('-'*10)\n",
    "print(\"Custom word tokenizer:\")\n",
    "print(word_tokenizer(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer with Stemmer or Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer with Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option1: NLTK PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def porter_stem_tokenizer(text:str)->list:\n",
    "  \"\"\"\n",
    "  Tokenize given text using custom word tokenizer\n",
    "  (based on NLTK word tokenizer) with Porter stemmer\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenizer(text)\n",
    "  stemmer = PorterStemmer()\n",
    "  stems = list()\n",
    "  for token in tokens:\n",
    "    stems.append(stemmer.stem(token))\n",
    "  return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option2: NLTK SnowballStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer as SnowballStemmer\n",
    "\n",
    "def snowball_stem_tokenizer(text:str)->list:\n",
    "  \"\"\"\n",
    "  Tokenize given text using custom word tokenizer\n",
    "  (based on NLTK word tokenizer) with Snowball stemmer\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenizer(text)\n",
    "  stemmer = SnowballStemmer()\n",
    "  stems = list()\n",
    "  for token in tokens:\n",
    "      stems.append(stemmer.stem(token))\n",
    "  return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option3: NLTK LancasterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def lancaster_stem_tokenizer(text):\n",
    "  \"\"\"\n",
    "  Tokenize given text using custom word tokenizer\n",
    "  (based on NLTK word tokenizer) with Lancaster stemmer\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenizer(text)\n",
    "  stemmer = LancasterStemmer()\n",
    "  stems = list()\n",
    "  for token in tokens:\n",
    "    stems.append(stemmer.stem(token))\n",
    "  return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer with Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option4: NLTK WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def wordnet_lemma_tokenizer(text:str)->list:\n",
    "  \"\"\"\n",
    "  Tokenize given text using custom word tokenizer\n",
    "  (based on NLTK word tokenizer) with Wordnet lemmatizer\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenizer(text)\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmas = list()\n",
    "  for token in tokens:\n",
    "    lemmas.append(lemmatizer.lemmatize(token))\n",
    "  return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option5: NLTK WordNet Lemmatizer with POS\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def convert_tag(tag:str)->str:\n",
    "  \"\"\"\n",
    "  Convert part-of-speech tag to tag compatible \n",
    "  with WordNet lemmatizer.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  tag : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  str\n",
    "    Part-of-speech tag compatible with WordNet lemmatizer; \n",
    "    \"n\" for noun, \"v\" for verb, \"a\" for adjective and \"r\" for adverb\n",
    "  \"\"\"\n",
    "  if tag[0] == 'V':\n",
    "    return 'v'\n",
    "  elif tag[0] == 'J':\n",
    "    return 'a'\n",
    "  elif tag[0] == 'R':\n",
    "    return 'r'\n",
    "  else:\n",
    "    return 'n'\n",
    "\n",
    "def wordnet_lemma_pos_tokenizer(text:str)->list:\n",
    "  \"\"\"\n",
    "  Tokenize given text using custom word tokenizer\n",
    "  (based on NLTK word tokenizer) with Wordnet lemmatizer\n",
    "  with predicting word's part-of-speech\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "    Text to be tokenized\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  list\n",
    "    List of tokens\n",
    "  \"\"\"\n",
    "  tokens = word_tokenizer(text)\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmas = list()\n",
    "  tokens_with_pos_tag = pos_tag(tokens)\n",
    "  for token in tokens_with_pos_tag:\n",
    "    word = token[0]\n",
    "    pos = convert_tag(token[1])\n",
    "    lemmas.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "  return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def term_weighting(tokenizer, all_texts):\n",
    "  \"\"\"\n",
    "  Parameters\n",
    "  ----------\n",
    "  tokenizer : list\n",
    "    A function of stemmer or lemmatizer method\n",
    "\n",
    "  all_text : list\n",
    "    List of all contents\n",
    "\n",
    "  Returns\n",
    "  ----------\n",
    "  matrix\n",
    "    Weighting matrix\n",
    "  \"\"\"\n",
    "  vectorizer = TfidfVectorizer(tokenizer=tokenizer,\n",
    "                              min_df=0.01)\n",
    "  term_weighted = vectorizer.fit_transform(all_texts)\n",
    "  return term_weighted\n",
    "\n",
    "text_feature = term_weighting(snowball_stem_tokenizer, raw_df['content'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 201)\t0.14244129734799157\n",
      "  (0, 309)\t0.08897912300765479\n",
      "  (0, 44)\t0.08345938057896157\n",
      "  (0, 484)\t0.08003683461572883\n",
      "  (0, 1696)\t0.08028131500609681\n",
      "  (0, 1876)\t0.08789440738794584\n",
      "  (0, 98)\t0.1047807037525534\n",
      "  (0, 498)\t0.06419690876833059\n",
      "  (0, 968)\t0.052560483873666956\n",
      "  (0, 1452)\t0.07203724890216268\n",
      "  (0, 2195)\t0.10727149909020334\n",
      "  (0, 465)\t0.09869699941605765\n",
      "  (0, 1052)\t0.09543712593368747\n",
      "  (0, 1299)\t0.10557881047868073\n",
      "  (0, 2092)\t0.07751904844067588\n",
      "  (0, 1053)\t0.0666303165043191\n",
      "  (0, 1585)\t0.06967636290471924\n",
      "  (0, 159)\t0.08587885268463721\n",
      "  (0, 680)\t0.10640822338111686\n",
      "  (0, 865)\t0.09151970527048509\n",
      "  (0, 2256)\t0.042679253220998\n",
      "  (0, 1778)\t0.06550051750294215\n",
      "  (0, 2272)\t0.032085422199505706\n",
      "  (0, 1686)\t0.08403879540541624\n",
      "  (0, 540)\t0.06908889962213145\n",
      "  :\t:\n",
      "  (1407, 1501)\t0.22675670283303906\n",
      "  (1407, 443)\t0.16195944583138722\n",
      "  (1407, 1758)\t0.08868296030521729\n",
      "  (1407, 903)\t0.059096269539404826\n",
      "  (1407, 1910)\t0.0952790086907031\n",
      "  (1407, 775)\t0.09589106355548772\n",
      "  (1407, 1351)\t0.09683845132820529\n",
      "  (1407, 1500)\t0.11041044730170294\n",
      "  (1407, 2024)\t0.06814871234314947\n",
      "  (1407, 1352)\t0.06457471274863265\n",
      "  (1407, 1274)\t0.07268815247121566\n",
      "  (1407, 1350)\t0.056498749962966\n",
      "  (1407, 999)\t0.06574031814411069\n",
      "  (1407, 1993)\t0.09036001269453382\n",
      "  (1407, 586)\t0.07634760045209968\n",
      "  (1407, 2200)\t0.05637260738741329\n",
      "  (1407, 582)\t0.0852287602202441\n",
      "  (1407, 490)\t0.06592460867781584\n",
      "  (1407, 1739)\t0.0290367150324823\n",
      "  (1407, 2202)\t0.07764867264562632\n",
      "  (1407, 865)\t0.1609066090500252\n",
      "  (1407, 2272)\t0.03760761299658485\n",
      "  (1407, 896)\t0.06405468746690772\n",
      "  (1407, 764)\t0.08962733976885896\n",
      "  (1407, 2066)\t0.04607198342851338\n"
     ]
    }
   ],
   "source": [
    "print(text_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_feature\n",
    "Y = raw_df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = []\n",
    "models.append(RandomForestClassifier())\n",
    "models.append(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def model_testing(models, feature, target):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list\n",
    "        List of sklearn's model\n",
    "\n",
    "    feature : matrix\n",
    "        Weighting matrix\n",
    "\n",
    "    target : pandas series\n",
    "        Target of data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    pandas dataframe\n",
    "        Result of cross validation with mean score\n",
    "    \"\"\"\n",
    "\n",
    "    cv_result = {\n",
    "        \"model_name\": [],\n",
    "        \"score\": []\n",
    "    }\n",
    "    X = feature\n",
    "    Y = target\n",
    "    for model in models:\n",
    "        cv_score = cross_val_score(model, X, Y, cv=5)\n",
    "        cv_result[\"model_name\"].append(str(model)[:-2])\n",
    "        cv_result[\"score\"].append(cv_score.mean())\n",
    "\n",
    "    result_df = pd.DataFrame(cv_result).sort_values(by=['score'], ascending=False)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.969453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.965907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name     score\n",
       "1  RandomForestClassifier  0.969453\n",
       "0    KNeighborsClassifier  0.965907"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test_result = model_testing(models, X, Y)\n",
    "model_test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameter\n",
    "params ={\n",
    "    \"knn_params\":{\n",
    "        'n_neighbors' : list(range(1, 16))\n",
    "    },\n",
    "    \"r2f_params\":{\n",
    "        'max_depth': list(range(2, 21, 2)),\n",
    "        'min_samples_split': list(range(2, 21, 2))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def parameter_tuning(models, params, feature, target):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    models : list\n",
    "        List of sklearn's model\n",
    "\n",
    "    params : dict\n",
    "        Dict of parameters depending on model\n",
    "\n",
    "    feature : matrix\n",
    "        Weighting matrix\n",
    "\n",
    "    target : pandas series\n",
    "        Target of data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    pandas dataframe\n",
    "        Result of GridSearchCV hyperparameter-tuning with best parameter and best score\n",
    "    \"\"\"\n",
    "    tuning_result = {\n",
    "        \"model_name\": [],\n",
    "        \"best_parameter\": [],\n",
    "        \"best_score\": []\n",
    "    }\n",
    "    X = feature\n",
    "    Y = target\n",
    "    for model, param in zip(models, params.values()):\n",
    "        clf = GridSearchCV(model, param, cv=5, n_jobs=-1, verbose=1) # 5 fold, n_jobs=-1 (use all core of processors)\n",
    "        result = clf.fit(X, Y)\n",
    "        tuning_result['model_name'].append(str(model)[:-2])\n",
    "        tuning_result['best_score'].append(result.best_score_)\n",
    "        tuning_result['best_parameter'].append(result.best_params_)\n",
    "\n",
    "    tuning_result = pd.DataFrame(tuning_result).sort_values(by=['best_score'], ascending=False)\n",
    "\n",
    "\n",
    "    return tuning_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>best_parameter</th>\n",
       "      <th>best_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'max_depth': 18, 'min_samples_split': 4}</td>\n",
       "      <td>0.974433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>{'n_neighbors': 7}</td>\n",
       "      <td>0.968751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name                             best_parameter  \\\n",
       "1  RandomForestClassifier  {'max_depth': 18, 'min_samples_split': 4}   \n",
       "0    KNeighborsClassifier                         {'n_neighbors': 7}   \n",
       "\n",
       "   best_score  \n",
       "1    0.974433  \n",
       "0    0.968751  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tuning_result = parameter_tuning(models, params, X, Y)\n",
    "model_tuning_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>score</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_parameter</th>\n",
       "      <th>increase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.969453</td>\n",
       "      <td>0.974433</td>\n",
       "      <td>{'max_depth': 18, 'min_samples_split': 4}</td>\n",
       "      <td>0.004980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.965907</td>\n",
       "      <td>0.968751</td>\n",
       "      <td>{'n_neighbors': 7}</td>\n",
       "      <td>0.002844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name     score  best_score  \\\n",
       "0  RandomForestClassifier  0.969453    0.974433   \n",
       "1    KNeighborsClassifier  0.965907    0.968751   \n",
       "\n",
       "                              best_parameter  increase  \n",
       "0  {'max_depth': 18, 'min_samples_split': 4}  0.004980  \n",
       "1                         {'n_neighbors': 7}  0.002844  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_result = pd.merge(model_test_result,\n",
    "                            model_tuning_result,\n",
    "                            how='inner',\n",
    "                            on='model_name')[['model_name', 'score', 'best_score', 'best_parameter']]\n",
    "compare_result['increase'] = compare_result['best_score'] - compare_result['score']\n",
    "compare_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d853cb24e8f7628e3ba29dc09518236c084a73039b3bc92092f3293b2fe13772"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
