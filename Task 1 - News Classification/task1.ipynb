{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Task 1\n",
    "\n",
    "โดย\n",
    "\n",
    "ตุลธร วงศ์ชัย รหัสนักศึกษา 63070224\n",
    "\n",
    "ธนภัทร ธีรรัตตัญญู รหัสนักศึกษา 63070227\n",
    "\n",
    "> **README**:\n",
    "> - 'Run all' is not recommended. \n",
    "> - Section 1 must be run before any other section.\n",
    "> - Read and follow instruction of each section carefully if it is provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Init Project\n",
    "\n",
    "> **Instruction**: Set `ROOT_PATH` before running this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Set Root Path\n",
    "\n",
    "* If you initialize this project for first time, run code cell R1\n",
    "* Otherwise, set your own root path in code cell R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Code Cell R1 ###\n",
    "\n",
    "# ROOT_PATH = os.getcwd() # Get current working directory\n",
    "\n",
    "# # Create /data directory\n",
    "# dir = os.path.join(ROOT_PATH, 'data')\n",
    "# os.mkdir(dir)\n",
    "\n",
    "# # Create /data/article_metadata directory\n",
    "# dir = os.path.join(ROOT_PATH, 'data/article_metadata')\n",
    "# os.mkdir(dir)\n",
    "\n",
    "# # Create /data/datastore directory\n",
    "# dir = os.path.join(ROOT_PATH, 'data/datastore')\n",
    "# os.mkdir(dir)\n",
    "\n",
    "# # Create /data/datastore/article_contents_by_month directory\n",
    "# dir = os.path.join(ROOT_PATH, 'data/datastore/article_contents_by_month')\n",
    "# os.mkdir(dir)\n",
    "\n",
    "# # Create /data/datastore/article_titles_by_month directory\n",
    "# dir = os.path.join(ROOT_PATH, 'data/datastore/article_titles_by_month')\n",
    "# os.mkdir(dir)\n",
    "\n",
    "# # Create /data/target directory\n",
    "# dir = os.path.join(ROOT_PATH, 'data/target')\n",
    "# os.mkdir(dir)\n",
    "\n",
    "# # Create /data/target/article_categories_by_month directory\n",
    "# dir = os.path.join(ROOT_PATH, 'data/target/article_categories_by_month')\n",
    "# os.mkdir(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell R2 ###\n",
    "\n",
    "# Set your own root path here\n",
    "ROOT_PATH = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Scraping from Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and packages used in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 กำหนดค่าคงที่และสร้างฟังก์ชันที่ใช้งานบ่อย"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1 กำหนดค่าคงที่ BASE_URL ของ News Article Archive ตามเซลล์โค้ดที่ 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 2.1.1 ###\n",
    "\n",
    "BASE_URL = 'http://www.it.kmitl.ac.th/~teerapong/news_archive/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 สร้างฟังก์ชันสำหรับส่ง GET request ไปยัง URL ที่ต้องการ พร้อมตรวจสอบว่าการส่งนั้นสำเร็จหรือไม่ (มี status เป็น 200) ถ้าไม่สำเร็จ ให้ทำการ raise Exception ตามเซลล์โค้ดที่ 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 2.1.2 ###\n",
    "\n",
    "def get_request(url: str, **kwargs) -> requests.models.Response:\n",
    "    \"\"\"\n",
    "    Call GET request using requests package \n",
    "    then return response if it is successful (status 200)\n",
    "    otherwise throw an exception.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "      URL to request\n",
    "    **kwargs\n",
    "      Other parameters for requests.get\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    requests.models.Response\n",
    "      Response of the request\n",
    "    \"\"\"\n",
    "    res = requests.get(url, **kwargs)\n",
    "\n",
    "    # If the request is not successful, throw an exception.\n",
    "    if (res.status_code != 200):\n",
    "        print(res.text)\n",
    "        err = Exception('The request is not successful. ' +\n",
    "                        f'Its status code is {res.status_code}. ' +\n",
    "                        f'The URL of request is {res.url}')\n",
    "        raise err\n",
    "    else:\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2 เก็บ URL ของหน้ารวมบทความข่าวแต่ละเดือน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 request หน้า index ของ News Article Archive และแปลงด้วย BeautifulSoup ตามเซลล์โค้ดที่ 2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 2.2.1 ###\n",
    "\n",
    "index_url = BASE_URL + 'index.html'\n",
    "res = get_request(index_url)\n",
    "index_page = BeautifulSoup(res.content, 'lxml')  # Parse the response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 เนื่องจากในหน้า index จะมีรายการของเดือนที่มีทั้งหมดอยู่ใน `<li>` ซึ่งเชื่อมไปยังหน้ารวมบทความข่าวของเดือนนั้น ๆ จึงทำการดึง `<li>` tag ตามเซลล์โค้ดที่ 2.2.2.1 จากนั้นทำการดึงค่าของ `href` ของ `<a>` ที่อยู่ในแต่ละ `<li>` tag ที่ได้มา ตามเซลล์โค้ดที่ 2.2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<li>Articles — <a href=\"month-jan-2017.html\">January</a> [118]</li>,\n",
       " <li>Articles — <a href=\"month-feb-2017.html\">February</a> [124]</li>,\n",
       " <li>Articles — <a href=\"month-mar-2017.html\">March</a> [116]</li>,\n",
       " <li>Articles — <a href=\"month-apr-2017.html\">April</a> [118]</li>,\n",
       " <li>Articles — <a href=\"month-may-2017.html\">May</a> [115]</li>,\n",
       " <li>Articles — <a href=\"month-jun-2017.html\">June</a> [115]</li>,\n",
       " <li>Articles — <a href=\"month-jul-2017.html\">July</a> [122]</li>,\n",
       " <li>Articles — <a href=\"month-aug-2017.html\">August</a> [116]</li>,\n",
       " <li>Articles — <a href=\"month-sep-2017.html\">September</a> [113]</li>,\n",
       " <li>Articles — <a href=\"month-oct-2017.html\">October</a> [124]</li>,\n",
       " <li>Articles — <a href=\"month-nov-2017.html\">November</a> [122]</li>,\n",
       " <li>Articles — <a href=\"month-dec-2017.html\">December</a> [115]</li>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code Cell 2.2.2.1 ###\n",
    "\n",
    "tags = index_page.select('li')\n",
    "tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month-jan-2017.html\n",
      "month-feb-2017.html\n",
      "month-mar-2017.html\n",
      "month-apr-2017.html\n",
      "month-may-2017.html\n",
      "month-jun-2017.html\n",
      "month-jul-2017.html\n",
      "month-aug-2017.html\n",
      "month-sep-2017.html\n",
      "month-oct-2017.html\n",
      "month-nov-2017.html\n",
      "month-dec-2017.html\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 2.2.2.2 ###\n",
    "\n",
    "month_links = list()\n",
    "for tag in tags:\n",
    "    link = tag.find('a')['href']\n",
    "    month_links.append(link)\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 เก็บข้อมูล metadata ของบทความข่าวในแต่ละเดือน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1 สร้างฟังก์ชันสำหรับใช้สกัดเอาข้อมูลชื่อเรื่อง หมวดหมู่ และลิงก์ ของแต่ละบทความข่าวในหน้ารวมบทความในเดือนหนึ่ง โดยจะเก็บเฉพาะบทความที่ยังเข้าถึงได้ ตามเซลล์โค้ดที่ 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 2.3.1 ###\n",
    "\n",
    "def scrape_month_page(month_url: str):\n",
    "    \"\"\"\n",
    "    Scrape title, category and link of available news articles from month page.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    month_url : str\n",
    "      URL of month page\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    DataFrame\n",
    "      DataFrame containing title, category and link of news articles\n",
    "    \"\"\"\n",
    "\n",
    "    res = get_request(month_url)\n",
    "    month_page = BeautifulSoup(res.content, 'lxml')  # Parse the response\n",
    "\n",
    "    # Each article's title, category, and link are in tr tag,\n",
    "    # so select all tr tag.\n",
    "    articles = month_page.select('tbody>tr')\n",
    "\n",
    "    article_titles = list()\n",
    "    article_categories = list()\n",
    "    article_links = list()\n",
    "\n",
    "    for article in articles:\n",
    "\n",
    "        # Category is in td tag with category class.\n",
    "        category = article.select_one('td.category').string.strip()\n",
    "\n",
    "        # Skip article if it is unavailable.\n",
    "        if (category != 'N/A'):\n",
    "            # Title is in a tag inside td tag with title class.\n",
    "            title = article.select_one('td.title>a').string\n",
    "            # Link to article page is href attribute of\n",
    "            # a tag inside td tag with title class.\n",
    "            link = article.select_one('td.title>a')['href']\n",
    "\n",
    "            article_titles.append(title)\n",
    "            article_categories.append(category)\n",
    "            article_links.append(link)\n",
    "\n",
    "    articles_df = pd.DataFrame({'title': article_titles,\n",
    "                                'category': article_categories,\n",
    "                                'link': article_links})\n",
    "    return articles_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2 เก็บข้อมูล metadata ของบทความ (หัวข้อ หมวดหมู่ และลิงก์) ในแต่ละเดือน โดยใช้ฟังก์ชันจากข้อ 2.3.1 และบันทึกเป็นไฟล์ csv แยกแต่ละเดือน ตามเซลล์โค้ดที่ 2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-jan-2017.html\n",
      "Scraped 118 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_01.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-feb-2017.html\n",
      "Scraped 122 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_02.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-mar-2017.html\n",
      "Scraped 116 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_03.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-apr-2017.html\n",
      "Scraped 117 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_04.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-may-2017.html\n",
      "Scraped 114 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_05.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-jun-2017.html\n",
      "Scraped 114 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_06.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-jul-2017.html\n",
      "Scraped 122 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_07.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-aug-2017.html\n",
      "Scraped 116 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_08.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-sep-2017.html\n",
      "Scraped 112 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_09.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-oct-2017.html\n",
      "Scraped 122 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_10.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-nov-2017.html\n",
      "Scraped 121 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_11.csv\n",
      "----------\n",
      "Scraping http://www.it.kmitl.ac.th/~teerapong/news_archive/month-dec-2017.html\n",
      "Scraped 114 article metadata\n",
      "Saved to ROOT_PATH/data/article_metadata/article_metadata_2017_12.csv\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 2.3.2 ###\n",
    "\n",
    "month_abbr_to_number = {\n",
    "    'jan': '01',\n",
    "    'feb': '02',\n",
    "    'mar': '03',\n",
    "    'apr': '04',\n",
    "    'may': '05',\n",
    "    'jun': '06',\n",
    "    'jul': '07',\n",
    "    'aug': '08',\n",
    "    'sep': '09',\n",
    "    'oct': '10',\n",
    "    'nov': '11',\n",
    "    'dec': '12'\n",
    "}\n",
    "\n",
    "for month_link in month_links:\n",
    "\n",
    "    month_url = BASE_URL + month_link\n",
    "    print(f'Scraping {month_url}')\n",
    "    articles_df = scrape_month_page(month_url)\n",
    "    print(f'Scraped {articles_df.shape[0]} article metadata')\n",
    "\n",
    "    month = month_link[6:9]  # Get month abbriviation from month_link\n",
    "    month = month_abbr_to_number[month]  # Convert to month number\n",
    "    year = month_link[10:14]  # Get year from month_link\n",
    "    file_name = f'article_metadata_{year}_{month}.csv'\n",
    "    file_path = f'{ROOT_PATH}/data/article_metadata/{file_name}'\n",
    "    articles_df.to_csv(file_path, index=False)\n",
    "    print(f'Saved to ROOT_PATH/data/article_metadata/{file_name}')\n",
    "    print('-'*10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 เก็บข้อมูลเนื้อความจากหน้าบทความข่าว"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.1 สร้างฟังก์ชันสำหรับใช้สกัดเอาข้อมูลเนื้อความจากหน้าบทความข่าว ตามเซลล์โค้ดที่ 2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 2.4.1 ###\n",
    "\n",
    "def scrape_article_page(article_url: str):\n",
    "    \"\"\"\n",
    "    Scrape article content from news article page with a given URL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    article_url : str\n",
    "      URL of article page\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    str\n",
    "      Article content\n",
    "    \"\"\"\n",
    "\n",
    "    res = get_request(article_url)\n",
    "    article_page = BeautifulSoup(res.content, 'lxml')  # Parse the response\n",
    "\n",
    "    # Get content of article, which is in p tags without 'notice' class\n",
    "    # in div.main\n",
    "    p_tags = article_page.select('div.main>p:not(.notice)')\n",
    "\n",
    "    # Every article repeats its title in content section.\n",
    "    # Due to inconsistent formating style, some articles repeat in b tag\n",
    "    # while some repeat in p tag. In the later case, remove first item of\n",
    "    # selected p_tags.\n",
    "    b_tags = article_page.select('div.main>b')\n",
    "    # If there is no b tag in content section, remove first item of\n",
    "    # selected p_tags\n",
    "    if (len(b_tags) == 0):\n",
    "        p_tags.pop(0)\n",
    "\n",
    "    article_content = ''\n",
    "    # Concatenate all paragraphs\n",
    "    for p in p_tags:\n",
    "        article_content += p.text + ' '\n",
    "\n",
    "    # Replace double space with single space and\n",
    "    # remove leading and trailing whitespace\n",
    "    article_content = article_content.replace('  ', ' ').strip()\n",
    "\n",
    "    return article_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 อ่านไฟล์ metadata ของบทความในแต่ละเดือน จากนั้นใช้ลิงก์ที่อยู่ใน metadata ไปทำการเก็บข้อมูลเนื้อความของบทความแต่ละบทความ โดยใช้ฟังก์ชันจากข้อ 2.4.1 และบันทึกเป็นไฟล์ txt แยกตามเดือน โดยที่เก็บเนื้อความของ 1 บทความต่อบรรทัด ตามเซลล์โค้ดที่ 2.4.2.1 - 2.4.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article_metadata_2017_01.csv',\n",
       " 'article_metadata_2017_02.csv',\n",
       " 'article_metadata_2017_03.csv',\n",
       " 'article_metadata_2017_04.csv',\n",
       " 'article_metadata_2017_05.csv',\n",
       " 'article_metadata_2017_06.csv',\n",
       " 'article_metadata_2017_07.csv',\n",
       " 'article_metadata_2017_08.csv',\n",
       " 'article_metadata_2017_09.csv',\n",
       " 'article_metadata_2017_10.csv',\n",
       " 'article_metadata_2017_11.csv',\n",
       " 'article_metadata_2017_12.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code Cell 2.4.2.1 ###\n",
    "\n",
    "article_metadata_month_files = os.listdir(f'{ROOT_PATH}/data/article_metadata')\n",
    "article_metadata_month_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 118 articles from article_metadata_2017_01.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_01.txt\n",
      "----------\n",
      "Scraping 122 articles from article_metadata_2017_02.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_02.txt\n",
      "----------\n",
      "Scraping 116 articles from article_metadata_2017_03.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_03.txt\n",
      "----------\n",
      "Scraping 117 articles from article_metadata_2017_04.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_04.txt\n",
      "----------\n",
      "Scraping 114 articles from article_metadata_2017_05.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_05.txt\n",
      "----------\n",
      "Scraping 114 articles from article_metadata_2017_06.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_06.txt\n",
      "----------\n",
      "Scraping 122 articles from article_metadata_2017_07.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_07.txt\n",
      "----------\n",
      "Scraping 116 articles from article_metadata_2017_08.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_08.txt\n",
      "----------\n",
      "Scraping 112 articles from article_metadata_2017_09.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_09.txt\n",
      "----------\n",
      "Scraping 122 articles from article_metadata_2017_10.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_10.txt\n",
      "----------\n",
      "Scraping 121 articles from article_metadata_2017_11.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_11.txt\n",
      "----------\n",
      "Scraping 114 articles from article_metadata_2017_12.csv\n",
      "Saved to ROOT_PATH/data/datastore/article_contents_by_month/article_contents_2017_12.txt\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 2.4.2.2 ###\n",
    "\n",
    "count = 0  # Request count\n",
    "\n",
    "# article_metadata_month_files is a list of metadata file names\n",
    "# from code cell 2.4.2.1\n",
    "for article_metadata_month in article_metadata_month_files:\n",
    "\n",
    "    articles_df = pd.read_csv(\n",
    "        f'{ROOT_PATH}/data/article_metadata/{article_metadata_month}')\n",
    "    print(\n",
    "        f'Scraping {articles_df.shape[0]} articles from {article_metadata_month}')\n",
    "    article_contents = ''\n",
    "\n",
    "    # Scrape content of each article\n",
    "    for _, article in articles_df.iterrows():\n",
    "        article_link = article['link']\n",
    "        article_url = BASE_URL + article_link\n",
    "        article_content = scrape_article_page(article_url)\n",
    "        article_contents += article_content + '\\n'\n",
    "\n",
    "        # Delay for 1 sec after 25 consecutive requests\n",
    "        count += 1\n",
    "        if (count % 25 == 0):\n",
    "            time.sleep(1)\n",
    "\n",
    "    article_contents = article_contents.rstrip('\\n')  # Remove last empty line\n",
    "\n",
    "    # Save contents in a file\n",
    "    year_month = article_metadata_month[17:24]  # Get year and month\n",
    "    file_name = f'article_contents_{year_month}.txt'\n",
    "    file_path = f'{ROOT_PATH}/data/datastore/article_contents_by_month/{file_name}'\n",
    "    f = open(file_path, mode='w', encoding='utf-8')  # Create a file\n",
    "    f.write(article_contents)  # Write a file\n",
    "    f.close()\n",
    "\n",
    "    print(\n",
    "        f'Saved to ROOT_PATH/data/datastore/article_contents_by_month/{file_name}')\n",
    "    print('-'*10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 จัดเก็บข้อมูลหัวข้อและหมวดหมู่ของบทความแยกจาก metadata ของบทความ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 อ่านไฟล์ metadata ของบทความในแต่ละเดือน จากนั้นดึงข้อมูลหัวข้อของบทความและบันทึกเป็นไฟล์ txt แยกตามเดือน โดยที่เก็บหัวข้อของ 1 บทความต่อบรรทัด และดึงข้อมูลหมวดหมู่ของบทความและบันทึกเป็นไฟล์ txt แยกตามเดือน โดยที่เก็บหมวดหมู่ของ 1 บทความต่อบรรทัด ตามเซลล์โค้ดที่ 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved titles and categories of 118 articles from article_metadata_2017_01.csv\n",
      "Saved titles and categories of 122 articles from article_metadata_2017_02.csv\n",
      "Saved titles and categories of 116 articles from article_metadata_2017_03.csv\n",
      "Saved titles and categories of 117 articles from article_metadata_2017_04.csv\n",
      "Saved titles and categories of 114 articles from article_metadata_2017_05.csv\n",
      "Saved titles and categories of 114 articles from article_metadata_2017_06.csv\n",
      "Saved titles and categories of 122 articles from article_metadata_2017_07.csv\n",
      "Saved titles and categories of 116 articles from article_metadata_2017_08.csv\n",
      "Saved titles and categories of 112 articles from article_metadata_2017_09.csv\n",
      "Saved titles and categories of 122 articles from article_metadata_2017_10.csv\n",
      "Saved titles and categories of 121 articles from article_metadata_2017_11.csv\n",
      "Saved titles and categories of 114 articles from article_metadata_2017_12.csv\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 2.5 ###\n",
    "\n",
    "# article_metadata_month_files is a list of metadata file names\n",
    "# from code cell 2.4.2.1\n",
    "for article_metadata_month in article_metadata_month_files:\n",
    "\n",
    "    articles_df = pd.read_csv(\n",
    "        f'{ROOT_PATH}/data/article_metadata/{article_metadata_month}')\n",
    "\n",
    "    # Get array of article titles then join with new line\n",
    "    article_titles = '\\n'.join(articles_df['title'].to_numpy())\n",
    "\n",
    "    # Get array of article categories then join with new line\n",
    "    article_categories = '\\n'.join(articles_df['category'].to_numpy())\n",
    "\n",
    "    year_month = article_metadata_month[17:24]  # Get year and month\n",
    "\n",
    "    # Save article titles in a file\n",
    "    file_name = f'article_titles_{year_month}.txt'\n",
    "    file_path = f'{ROOT_PATH}/data/datastore/article_titles_by_month/{file_name}'\n",
    "    with open(file_path, mode='w', encoding='utf-8') as file:\n",
    "        file.write(article_titles)\n",
    "\n",
    "    # Save article categories in a file\n",
    "    file_name = f'article_categories_{year_month}.txt'\n",
    "    file_path = f'{ROOT_PATH}/data/target/article_categories_by_month/{file_name}'\n",
    "    with open(file_path, mode='w', encoding='utf-8') as file:\n",
    "        file.write(article_categories)\n",
    "\n",
    "    print(f'Saved titles and categories of {articles_df.shape[0]} ' +\n",
    "          f'articles from {article_metadata_month}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 รวมไฟล์ข้อมูลหัวข้อ เนื้อความ และหมวดหมู่ของบทความในทุกเดือน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6.1 สร้างฟังก์ชันสำหรับรวมไฟล์ ตามเซลล์โค้ดที่ 2.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 2.6.1 ###\n",
    "\n",
    "def merge_text_files(file_paths: list, out_file_path: str):\n",
    "    \"\"\"\n",
    "    Merge text files in the same order as a given file path list\n",
    "    then write an out file with a given path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_paths : list\n",
    "      List of text file paths\n",
    "    out_file_path : str\n",
    "      File path to write a merged text file\n",
    "    \"\"\"\n",
    "\n",
    "    data = ''\n",
    "\n",
    "    for i in range(len(file_paths)):\n",
    "        with open(file_paths[i], mode='r', encoding='utf-8') as file:\n",
    "            data += file.read()  # Append content of current text file to data\n",
    "\n",
    "        # Add new line to data if it is not the last text file\n",
    "        if (i < len(file_paths) - 1):\n",
    "            data += '\\n'\n",
    "\n",
    "    # Write a merged text file\n",
    "    with open(out_file_path, mode='w', encoding='utf-8') as file:\n",
    "        file.write(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6.2 รวมไฟล์ข้อมูลหัวข้อของบทความ โดยใช้ฟังก์ชันจากข้อ 2.6.1 ตามเซลล์โค้ดที่ 2.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed merging article title files\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 2.6.2 ###\n",
    "\n",
    "# Get a list of file names in article_titles_by_month directory.\n",
    "article_titles_by_month_dir = f'{ROOT_PATH}/data/datastore/article_titles_by_month'\n",
    "article_titles_file_names = os.listdir(article_titles_by_month_dir)\n",
    "\n",
    "# Add directory path to file name.\n",
    "article_titles_file_paths = [\n",
    "    article_titles_by_month_dir + '/' + file_name\n",
    "    for file_name in article_titles_file_names\n",
    "]\n",
    "\n",
    "article_titles_all_file_path = f'{ROOT_PATH}/data/datastore/article_titles_all.txt'\n",
    "\n",
    "merge_text_files(article_titles_file_paths, article_titles_all_file_path)\n",
    "print('Completed merging article title files')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6.3 รวมไฟล์ข้อมูลเนื้อความของบทความ โดยใช้ฟังก์ชันจากข้อ 2.6.1 ตามเซลล์โค้ดที่ 2.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed merging article content files\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 2.6.3 ###\n",
    "\n",
    "# Get a list of file names in article_contents_by_month directory.\n",
    "article_contents_by_month_dir = f'{ROOT_PATH}/data/datastore/article_contents_by_month'\n",
    "article_contents_file_names = os.listdir(article_contents_by_month_dir)\n",
    "\n",
    "# Add directory path to file name.\n",
    "article_contents_file_paths = [\n",
    "    article_contents_by_month_dir + '/' + file_name\n",
    "    for file_name in article_contents_file_names\n",
    "]\n",
    "\n",
    "article_contents_all_file_path = f'{ROOT_PATH}/data/datastore/article_contents_all.txt'\n",
    "\n",
    "merge_text_files(article_contents_file_paths, article_contents_all_file_path)\n",
    "print('Completed merging article content files')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6.4 รวมไฟล์ข้อมูลหมวดหมู่ของบทความ โดยใช้ฟังก์ชันจากข้อ 2.6.1 ตามเซลล์โค้ดที่ 2.6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed merging article catagory files\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 2.6.4 ###\n",
    "\n",
    "# Get a list of file names in article_categories_by_month directory.\n",
    "article_categories_by_month_dir = f'{ROOT_PATH}/data/target/article_categories_by_month'\n",
    "article_categories_file_names = os.listdir(article_categories_by_month_dir)\n",
    "\n",
    "# Add directory path to file name.\n",
    "article_categories_file_paths = [\n",
    "    article_categories_by_month_dir + '/' + file_name\n",
    "    for file_name in article_categories_file_names\n",
    "]\n",
    "\n",
    "article_categories_all_file_path = f'{ROOT_PATH}/data/target/article_categories_all.txt'\n",
    "\n",
    "merge_text_files(article_categories_file_paths,\n",
    "                 article_categories_all_file_path)\n",
    "print('Completed merging article catagory files')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 รวมข้อมูลหัวข้อและเนื้อความของบทความเข้าด้วยกัน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7 อ่านไฟล์ข้อมูลหัวข้อของบทความและไฟล์ข้อมูลเนื้อความของบทความ แล้วนำหัวข้อต่อกับเนื้อความของแต่ละบทความโดยคั่นด้วยเว้นวรรค และบันทึกเป็นไฟล์ txt โดยที่เก็บหัวข้อที่ต่อด้วยเนื้อความของ 1 บทความต่อบรรทัด ตามเซลล์โค้ดที่ 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed merging article_titles_all.txt and article_contents_all.txt\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 2.7 ###\n",
    "\n",
    "# Define article title file path and article content file path to read\n",
    "titles_file_path = f'{ROOT_PATH}/data/datastore/article_titles_all.txt'\n",
    "contents_file_path = f'{ROOT_PATH}/data/datastore/article_contents_all.txt'\n",
    "\n",
    "# Define file path to write\n",
    "out_file_name = 'article_titles_plus_contents_all.txt'\n",
    "out_file_path = f'{ROOT_PATH}/data/datastore/{out_file_name}'\n",
    "\n",
    "# Read article title file and article content file\n",
    "# and merge them line by line into an out file.\n",
    "with open(titles_file_path, mode='r', encoding='utf-8') as titles_file:\n",
    "    with open(contents_file_path, mode='r', encoding='utf-8') as contents_file:\n",
    "        with open(out_file_path, mode='w', encoding='utf-8') as out_file:\n",
    "            titles = titles_file.readlines()\n",
    "            contents = contents_file.readlines()\n",
    "\n",
    "            titles_plus_contents = ''\n",
    "\n",
    "            # Merge titles and contents line by line\n",
    "            for title, content in zip(titles, contents):\n",
    "                title_plus_content = title.rstrip(\n",
    "                    '\\n') + ' ' + content.rstrip('\\n')\n",
    "                titles_plus_contents += title_plus_content + '\\n'\n",
    "\n",
    "            # Remove last empty line\n",
    "            titles_plus_contents = titles_plus_contents.rstrip('\\n')\n",
    "\n",
    "            # Write title plus content file\n",
    "            out_file.write(titles_plus_contents)\n",
    "\n",
    "print('Completed merging article_titles_all.txt and article_contents_all.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-Class Classification\n",
    "\n",
    "> **Instruction**: Scraped data from section 2 must exist in `ROOT_PATH/data` before running this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and packages used in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Preprocessing ###\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer as SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classification ###\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 อ่านข้อมูลจากการทำ Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.1 อ่านข้อมูลเนื้อความของบทความ ตามเซลล์โค้ดที่ 3.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.1.1 ###\n",
    "\n",
    "with open(f'{ROOT_PATH}/data/datastore/article_contents_all.txt',\n",
    "          mode='r', encoding='utf-8') as file:\n",
    "    contents = file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.2 อ่านข้อมูลหัวข้อต่อกับเนื้อความของบทความ ตามเซลล์โค้ดที่ 3.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.1.2 ###\n",
    "\n",
    "with open(f'{ROOT_PATH}/data/datastore/article_titles_plus_contents_all.txt',\n",
    "          mode='r', encoding='utf-8') as file:\n",
    "    contents_with_title = file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.3 อ่านข้อมูลหมวดหมู่ของบทความ ตามเซลล์โค้ดที่ 3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.1.3 ###\n",
    "\n",
    "with open(f'{ROOT_PATH}/data/target/article_categories_all.txt',\n",
    "          mode='r', encoding='utf-8') as file:\n",
    "    targets = file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Basic Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.1.1 กำหนด STOPS_WORDS โดยรวมเซ็ทคำ stop word ของทั้ง NLTK และ Sklearn เข้าด้วยกัน ตามเซลล์โค้ดที่ 3.2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.2.1.1 ###\n",
    "\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "SKLEARN_STOP_WORDS = ENGLISH_STOP_WORDS\n",
    "\n",
    "# Merge stop words from ntlk and sklearn\n",
    "STOP_WORDS = NLTK_STOP_WORDS.union(SKLEARN_STOP_WORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.1.2 สร้าง tokenizer โดยนำเอา word tokenizer จาก NLTK มาเพิ่มการทำ normalizing และการกรองเอาคำ stop words ตัวเลข และเครื่องหมายวรรคตอน (punctuation mark) ออก ตามเซลล์โค้ดที่ 3.2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.2.1.2 ###\n",
    "\n",
    "def word_tokenizer(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize given text using NLTK's word tokenizer\n",
    "    with normalizing (lowercasing string) and filtering \n",
    "    stop words, numbers and punctuation marks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "      Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "      List of tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    tokens_to_return = list()\n",
    "    for token in tokens:\n",
    "        token = token.strip(\"'\")\n",
    "\n",
    "        # Filter number\n",
    "        if (re.match(r\"^[\\d.]+$\", token)):\n",
    "            continue\n",
    "        # Filter punctuation mark and stop word\n",
    "        elif (re.match(r\"[\\w'-]+\", token) and (token not in ['-', \"'\"])\n",
    "              and (token not in STOP_WORDS)):\n",
    "            tokens_to_return.append(token)\n",
    "\n",
    "    return tokens_to_return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.1.3 นำ tokenizer ที่ได้มาทดสอบ ตามเซลล์โค้ดที่ 3.2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "It's 21-century education. \n",
      "They are Mr. and Mrs. Brown. \n",
      "While this seems like a cliché, it is true. \n",
      "6.80 pounds :; or £6.80\n",
      "— em dash – en dash - hyphen\n",
      "----------\n",
      "NLTK's word tokenizer:\n",
      "['It', \"'s\", '21-century', 'education', '.', 'They', 'are', 'Mr.', 'and', 'Mrs.', 'Brown', '.', 'While', 'this', 'seems', 'like', 'a', 'cliché', ',', 'it', 'is', 'true', '.', '6.80', 'pounds', ':', ';', 'or', '£6.80', '—', 'em', 'dash', '–', 'en', 'dash', '-', 'hyphen']\n",
      "----------\n",
      "Custom word tokenizer:\n",
      "['21-century', 'education', 'mr.', 'mrs.', 'brown', 'like', 'cliché', 'true', 'pounds', 'em', 'dash', 'en', 'dash', 'hyphen']\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 3.2.1.3 ###\n",
    "\n",
    "test_text = \"\"\"It's 21-century education. \n",
    "They are Mr. and Mrs. Brown. \n",
    "While this seems like a cliché, it is true. \n",
    "6.80 pounds :; or £6.80\n",
    "— em dash – en dash - hyphen\"\"\"\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print('-'*10)\n",
    "print(\"NLTK's word tokenizer:\")\n",
    "print(word_tokenize(test_text))\n",
    "print('-'*10)\n",
    "print(\"Custom word tokenizer:\")\n",
    "print(word_tokenizer(test_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Tokenizer with Stemmer or Lemmatizer\n",
    "\n",
    "นำ tokenizer ที่สร้างขึ้นจากข้อ 3.2.1 มาเพิ่ม stemmer หรือ lemmatizer เข้าไป"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2.1 สร้าง tokenizer โดยเพิ่ม Porter stemmer ตามเซลล์โค้ดที่ 3.2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.2.2.1 ###\n",
    "\n",
    "def porter_stem_tokenizer(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize given text using custom word tokenizer\n",
    "    (based on NLTK word tokenizer) with Porter stemmer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "      Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "      List of tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenizer(text)\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = list()\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2.2 สร้าง tokenizer โดยเพิ่ม Porter stemmer ตามเซลล์โค้ดที่ 3.2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.2.2.2 ###\n",
    "\n",
    "def snowball_stem_tokenizer(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize given text using custom word tokenizer\n",
    "    (based on NLTK word tokenizer) with Snowball stemmer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "      Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "      List of tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenizer(text)\n",
    "    stemmer = SnowballStemmer()\n",
    "    stems = list()\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2.3 สร้าง tokenizer โดยเพิ่ม Lancaster stemmer ตามเซลล์โค้ดที่ 3.2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.2.2.3 ###\n",
    "\n",
    "def lancaster_stem_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Tokenize given text using custom word tokenizer\n",
    "    (based on NLTK word tokenizer) with Lancaster stemmer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "      Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "      List of tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenizer(text)\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = list()\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2.4 สร้าง tokenizer โดยเพิ่ม Wordnet lemmatizer ตามเซลล์โค้ดที่ 3.2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.2.2.4 ###\n",
    "\n",
    "def wordnet_lemma_tokenizer(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize given text using custom word tokenizer\n",
    "    (based on NLTK word tokenizer) with Wordnet lemmatizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "      Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "      List of tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenizer(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = list()\n",
    "    for token in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(token))\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.2.5 สร้าง tokenizer โดยเพิ่ม Wordnet lemmatizer with POS ตามเซลล์โค้ดที่ 3.2.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.2.2.5 ###\n",
    "\n",
    "def convert_tag(tag: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert part-of-speech tag to tag compatible \n",
    "    with WordNet lemmatizer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : str\n",
    "      Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    str\n",
    "      Part-of-speech tag compatible with WordNet lemmatizer; \n",
    "      \"n\" for noun, \"v\" for verb, \"a\" for adjective and \"r\" for adverb\n",
    "    \"\"\"\n",
    "    if tag[0] == 'V':\n",
    "        return 'v'\n",
    "    elif tag[0] == 'J':\n",
    "        return 'a'\n",
    "    elif tag[0] == 'R':\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "def wordnet_lemma_pos_tokenizer(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize given text using custom word tokenizer\n",
    "    (based on NLTK word tokenizer) with Wordnet lemmatizer\n",
    "    given word's part-of-speech\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "      Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "      List of tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenizer(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = list()\n",
    "    tokens_with_pos_tag = pos_tag(tokens)\n",
    "    for token in tokens_with_pos_tag:\n",
    "        word = token[0]\n",
    "        pos = convert_tag(token[1])\n",
    "        lemmas.append(lemmatizer.lemmatize(word, pos=pos))\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Vectorizing Raw Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.3.1 กำหนด list ของ tokenizer แบบต่างๆ ที่ได้สร้างขึ้นจากข้อ 3.2.1 และ 3.2.2 ตามเซลล์โค้ดที่ 3.2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.2.3.1 ###\n",
    "\n",
    "tokenizers = [word_tokenizer, porter_stem_tokenizer,\n",
    "              snowball_stem_tokenizer, lancaster_stem_tokenizer,\n",
    "              wordnet_lemma_tokenizer, wordnet_lemma_pos_tokenizer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.3.2 ทำการ tokenize ข้อความเนื้อความของบทความ (contents) ด้วย tokenizer แบบต่างๆ จากนั้นนำไปเข้า TFIDF vectorizer เพื่อทำการ term weighting โดยที่กำหนด min_df อยู่ที่ 0.1 ตามเซลล์โค้ดที่ 3.2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing with word_tokenizer\n",
      "Vectorizing with porter_stem_tokenizer\n",
      "Vectorizing with snowball_stem_tokenizer\n",
      "Vectorizing with lancaster_stem_tokenizer\n",
      "Vectorizing with wordnet_lemma_tokenizer\n",
      "Vectorizing with wordnet_lemma_pos_tokenizer\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 3.2.3.2 ###\n",
    "\n",
    "contents_X = dict()\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "    print(f'Vectorizing with {tokenizer.__name__}')\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenizer, min_df=0.01)\n",
    "    term_weighted = vectorizer.fit_transform(contents)\n",
    "    contents_X[tokenizer.__name__] = term_weighted.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.3.3 ทำการ tokenize ข้อความหัวข้อต่อกับเนื้อความของบทความ (contents_with_title) ด้วย tokenizer แบบต่างๆ จากนั้นนำไปเข้า TFIDF vectorizer เพื่อทำการ term weighting โดยที่กำหนด min_df อยู่ที่ 0.1 ตามเซลล์โค้ดที่ 3.2.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing with word_tokenizer\n",
      "Vectorizing with porter_stem_tokenizer\n",
      "Vectorizing with snowball_stem_tokenizer\n",
      "Vectorizing with lancaster_stem_tokenizer\n",
      "Vectorizing with wordnet_lemma_tokenizer\n",
      "Vectorizing with wordnet_lemma_pos_tokenizer\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 3.2.3.2 ###\n",
    "\n",
    "contents_with_title_X = dict()\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "    print(f'Vectorizing with {tokenizer.__name__}')\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenizer, min_df=0.01)\n",
    "    term_weighted = vectorizer.fit_transform(contents_with_title)\n",
    "    contents_with_title_X[tokenizer.__name__] = term_weighted.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Classification 1\n",
    "\n",
    "นำข้อมูลเนื้อความของบทความ (contents_X) ที่เตรียมไว้จากข้อ 3.2.3.2 มาทำการทดลองกับโมเดล KNN, RandomForest และ LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.1 กำหนด model และช่วงของค่า parameter ที่ต้องการทำ tuning ตามเซลล์โค้ดที่ 3.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.3.1 ###\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\"n_neighbors\": list(range(3, 16, 2))}\n",
    "    },\n",
    "    {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            \"C\": list(np.arange(0.1, 1.1, 0.1))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": list(range(25, 201, 25)),\n",
    "            \"min_samples_split\": list(range(5, 26, 5)),\n",
    "            \"random_state\": [28]\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.2 ทำการ parameter tuning หา parameter ที่ให้ค่า accuracy ที่ดีที่สุดด้วย 5-fold cross validation (GridSearchCV) ของคู่ model กับข้อมูลที่ผ่านการเตรียมด้วย tokenizer แบบต่างๆ จากนั้นนำมาทดสอบกับ test set เพื่อประเมินประสิทธิภาพ model ตามเซลล์โค้ดที่ 3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs tokenized by word_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9597, params {'n_neighbors': 15}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9799, params {'C': 0.4, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9703, params {'min_samples_split': 15, 'n_estimators': 125, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by porter_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9682, params {'n_neighbors': 9}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9820, params {'C': 0.5, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9724, params {'min_samples_split': 10, 'n_estimators': 75, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by snowball_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9682, params {'n_neighbors': 9}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9830, params {'C': 0.8, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9703, params {'min_samples_split': 10, 'n_estimators': 100, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by lancaster_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9735, params {'n_neighbors': 13}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9799, params {'C': 0.6, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9767, params {'min_samples_split': 5, 'n_estimators': 100, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by wordnet_lemma_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9661, params {'n_neighbors': 15}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9798, params {'C': 0.6, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9724, params {'min_samples_split': 20, 'n_estimators': 100, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by wordnet_lemma_pos_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9692, params {'n_neighbors': 15}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9820, params {'C': 0.8, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9714, params {'min_samples_split': 5, 'n_estimators': 75, 'random_state': 28}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 3.3.2 ###\n",
    "\n",
    "results1 = {\n",
    "    \"model\": [],\n",
    "    \"tokenizer\": [],\n",
    "    \"best_tuning_params\": [],\n",
    "    \"best_tuning_acc\": [],\n",
    "    \"accuracy\":[],\n",
    "    \"f1_score\":[],\n",
    "}\n",
    "\n",
    "y = targets\n",
    "\n",
    "for tokenizer_name, X in contents_X.items():\n",
    "    print(f'Docs tokenized by {tokenizer_name}')\n",
    "\n",
    "    # Stratified train-test split with test size of 33%\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "        random_state=50, test_size=.33, stratify=y) \n",
    "\n",
    "    for model in models:\n",
    "        print(f\"- Tuning {type(model['model']).__name__}:\")\n",
    "\n",
    "        # Search for best set of parameters, using 5-fold cross validation\n",
    "        clf = GridSearchCV(model['model'], model['params'], cv=5, n_jobs=-1, verbose=1)\n",
    "        result = clf.fit(X_train, y_train)\n",
    "        results1['model'].append(type(model['model']).__name__)\n",
    "        results1['tokenizer'].append(tokenizer_name)\n",
    "        results1['best_tuning_params'].append(result.best_params_)\n",
    "        results1['best_tuning_acc'].append(result.best_score_)\n",
    "        print(f'Best tuning: acc {result.best_score_:.4f}, params {result.best_params_}')\n",
    "\n",
    "        # Test model with best set of parameters from GridSearchCV\n",
    "        model = model['model']\n",
    "        model.set_params(**result.best_params_)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results1['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        results1['f1_score'].append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3.3 แสดงผลลัพธ์จากข้อ 3.3.2 ตามเซลล์โค้ดที่ 3.3.3.1 - 3.3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.980645</td>\n",
       "      <td>0.980610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.976344</td>\n",
       "      <td>0.976311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.976344</td>\n",
       "      <td>0.976298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.974177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.974128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.974238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.972001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.972033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.971985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.972033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.971971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.969892</td>\n",
       "      <td>0.969963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.969892</td>\n",
       "      <td>0.969873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.969892</td>\n",
       "      <td>0.969745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>0.965658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>0.965461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>0.965658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>0.965674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model                    tokenizer  accuracy  f1_score\n",
       "0   RandomForestClassifier     lancaster_stem_tokenizer  0.980645  0.980610\n",
       "1   RandomForestClassifier      snowball_stem_tokenizer  0.976344  0.976311\n",
       "2       LogisticRegression               word_tokenizer  0.976344  0.976298\n",
       "3   RandomForestClassifier  wordnet_lemma_pos_tokenizer  0.974194  0.974177\n",
       "4       LogisticRegression        porter_stem_tokenizer  0.974194  0.974128\n",
       "5     KNeighborsClassifier  wordnet_lemma_pos_tokenizer  0.974194  0.974238\n",
       "6   RandomForestClassifier               word_tokenizer  0.972043  0.972001\n",
       "7       LogisticRegression  wordnet_lemma_pos_tokenizer  0.972043  0.972033\n",
       "8       LogisticRegression      wordnet_lemma_tokenizer  0.972043  0.971985\n",
       "9       LogisticRegression      snowball_stem_tokenizer  0.972043  0.972033\n",
       "10      LogisticRegression     lancaster_stem_tokenizer  0.972043  0.971971\n",
       "11    KNeighborsClassifier      wordnet_lemma_tokenizer  0.969892  0.969963\n",
       "12    KNeighborsClassifier               word_tokenizer  0.969892  0.969873\n",
       "13  RandomForestClassifier        porter_stem_tokenizer  0.969892  0.969745\n",
       "14    KNeighborsClassifier      snowball_stem_tokenizer  0.965591  0.965658\n",
       "15  RandomForestClassifier      wordnet_lemma_tokenizer  0.965591  0.965461\n",
       "16    KNeighborsClassifier        porter_stem_tokenizer  0.965591  0.965658\n",
       "17    KNeighborsClassifier     lancaster_stem_tokenizer  0.965591  0.965674"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code Cell 3.3.3.1 ###\n",
    "\n",
    "result_df = pd.DataFrame(results1)\n",
    "\n",
    "result_df[['model','tokenizer','accuracy','f1_score']] \\\n",
    "    .sort_values(by=['accuracy'], ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากผลลัพธ์ของเซลล์โค้ดที่ 3.3.3.1 การจำแนกข้อความเนื้อความของบทความ (contents) ที่ทำการ preprocess โดยใช้ lancaster_stem_tokenizer ด้วย RandomForestClassifier ให้ประสิทธิภาพดีที่สุด ซึ่งมี accuracy อยู่ที่ 98.0645% และ f1 score อยู่ที่ 0.980610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.980645</td>\n",
       "      <td>0.980610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.976344</td>\n",
       "      <td>0.976311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.976344</td>\n",
       "      <td>0.976298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.974177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.974128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.971985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tokenizer                   model  accuracy  f1_score\n",
       "0     lancaster_stem_tokenizer  RandomForestClassifier  0.980645  0.980610\n",
       "1      snowball_stem_tokenizer  RandomForestClassifier  0.976344  0.976311\n",
       "2               word_tokenizer      LogisticRegression  0.976344  0.976298\n",
       "3  wordnet_lemma_pos_tokenizer  RandomForestClassifier  0.974194  0.974177\n",
       "4        porter_stem_tokenizer      LogisticRegression  0.974194  0.974128\n",
       "5      wordnet_lemma_tokenizer      LogisticRegression  0.972043  0.971985"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code Cell 3.3.3.2 ###\n",
    "\n",
    "result_df[['tokenizer','model','accuracy','f1_score']] \\\n",
    "    .sort_values(by=['accuracy'], ascending=False) \\\n",
    "    .groupby('tokenizer') \\\n",
    "    .head(1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากผลลัพธ์ของเซลล์โค้ดที่ 3.3.3.2 พบว่าจำแนกข้อความเนื้อความของบทความ (contents) ที่ทำการ preprocess โดยใช้ lancaster_stem_tokenizer, snowball_stem_tokenizer, word_tokenizer, wordnet_lemma_pos_tokenizer, porter_stem_tokenizer, wordnet_lemma_tokenizer ได้ดีที่สุดตามลำดับ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Classification 2\n",
    "\n",
    "นำข้อมูลหัวข้อต่อกับเนื้อความของบทความ (contents_with_title_X) ที่เตรียมไว้จากข้อ 3.2.3.3 มาทำการทดลองกับโมเดล KNN, RandomForest และ LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4.1 กำหนด model และช่วงของค่า parameter ที่ต้องการทำ tuning ตามเซลล์โค้ดที่ 3.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.4.1 ###\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\"n_neighbors\": list(range(3, 16, 2))}\n",
    "    },\n",
    "    {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            \"C\": list(np.arange(0.1, 1.1, 0.1))\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": list(range(25, 201, 25)),\n",
    "            \"min_samples_split\": list(range(2, 26, 5)),\n",
    "            \"random_state\": [28]\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4.2 ทำการ parameter tuning หา parameter ที่ให้ค่า accuracy ที่ดีที่สุดด้วย 5-fold cross validation (GridSearchCV) ของคู่ model กับข้อมูลที่ผ่านการเตรียมด้วย tokenizer แบบต่างๆ จากนั้นนำมาทดสอบกับ test set เพื่อประเมินประสิทธิภาพ model ตามเซลล์โค้ดที่ 3.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs tokenized by word_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9576, params {'n_neighbors': 15}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9788, params {'C': 0.5, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9746, params {'min_samples_split': 7, 'n_estimators': 200, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by porter_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9682, params {'n_neighbors': 9}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9830, params {'C': 0.8, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9725, params {'min_samples_split': 2, 'n_estimators': 150, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by snowball_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9661, params {'n_neighbors': 11}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9830, params {'C': 0.8, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9714, params {'min_samples_split': 7, 'n_estimators': 125, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by lancaster_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9682, params {'n_neighbors': 9}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9809, params {'C': 0.9, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9746, params {'min_samples_split': 17, 'n_estimators': 100, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by wordnet_lemma_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9618, params {'n_neighbors': 9}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9820, params {'C': 1.0, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9682, params {'min_samples_split': 17, 'n_estimators': 100, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by wordnet_lemma_pos_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9671, params {'n_neighbors': 9}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9830, params {'C': 0.8, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9735, params {'min_samples_split': 2, 'n_estimators': 100, 'random_state': 28}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 3.4.2 ###\n",
    "\n",
    "results2 = {\n",
    "    \"model\": [],\n",
    "    \"tokenizer\": [],\n",
    "    \"best_tuning_params\": [],\n",
    "    \"best_tuning_acc\": [],\n",
    "    \"accuracy\":[],\n",
    "    \"f1_score\":[],\n",
    "}\n",
    "\n",
    "y = targets\n",
    "\n",
    "for tokenizer_name, X in contents_with_title_X.items():\n",
    "    print(f'Docs tokenized by {tokenizer_name}')\n",
    "\n",
    "    # Stratified train-test split with test size of 33%\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "        random_state=50, test_size=.33, stratify=y) \n",
    "\n",
    "    for model in models:\n",
    "        print(f\"- Tuning {type(model['model']).__name__}:\")\n",
    "\n",
    "        # Search for best set of parameters, using 5-fold cross validation\n",
    "        clf = GridSearchCV(model['model'], model['params'], cv=5, n_jobs=-1, verbose=1)\n",
    "        result = clf.fit(X_train, y_train)\n",
    "        results2['model'].append(type(model['model']).__name__)\n",
    "        results2['tokenizer'].append(tokenizer_name)\n",
    "        results2['best_tuning_params'].append(result.best_params_)\n",
    "        results2['best_tuning_acc'].append(result.best_score_)\n",
    "        print(f'Best tuning: acc {result.best_score_:.4f}, params {result.best_params_}')\n",
    "\n",
    "        # Test model with best set of parameters from GridSearchCV\n",
    "        model = model['model']\n",
    "        model.set_params(**result.best_params_)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results2['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        results2['f1_score'].append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4.3 แสดงผลลัพธ์จากข้อ 3.4.2 ตามเซลล์โค้ดที่ 3.4.3.1 - 3.4.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.978495</td>\n",
       "      <td>0.978457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.976344</td>\n",
       "      <td>0.976329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.976344</td>\n",
       "      <td>0.976336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.974194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.974167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.974155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.972001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.972012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.972012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.972012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.967645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.967617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.967761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>0.965621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.965591</td>\n",
       "      <td>0.965567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.963441</td>\n",
       "      <td>0.963504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.961290</td>\n",
       "      <td>0.961387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.959140</td>\n",
       "      <td>0.959131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model                    tokenizer  accuracy  f1_score\n",
       "0   RandomForestClassifier     lancaster_stem_tokenizer  0.978495  0.978457\n",
       "1   RandomForestClassifier  wordnet_lemma_pos_tokenizer  0.976344  0.976329\n",
       "2       LogisticRegression     lancaster_stem_tokenizer  0.976344  0.976336\n",
       "3       LogisticRegression        porter_stem_tokenizer  0.974194  0.974194\n",
       "4   RandomForestClassifier      snowball_stem_tokenizer  0.974194  0.974167\n",
       "5       LogisticRegression               word_tokenizer  0.974194  0.974155\n",
       "6   RandomForestClassifier               word_tokenizer  0.972043  0.972001\n",
       "7       LogisticRegression  wordnet_lemma_pos_tokenizer  0.972043  0.972012\n",
       "8       LogisticRegression      snowball_stem_tokenizer  0.972043  0.972012\n",
       "9       LogisticRegression      wordnet_lemma_tokenizer  0.972043  0.972012\n",
       "10  RandomForestClassifier      wordnet_lemma_tokenizer  0.967742  0.967645\n",
       "11  RandomForestClassifier        porter_stem_tokenizer  0.967742  0.967617\n",
       "12    KNeighborsClassifier      wordnet_lemma_tokenizer  0.967742  0.967761\n",
       "13    KNeighborsClassifier  wordnet_lemma_pos_tokenizer  0.965591  0.965621\n",
       "14    KNeighborsClassifier               word_tokenizer  0.965591  0.965567\n",
       "15    KNeighborsClassifier      snowball_stem_tokenizer  0.963441  0.963504\n",
       "16    KNeighborsClassifier     lancaster_stem_tokenizer  0.961290  0.961387\n",
       "17    KNeighborsClassifier        porter_stem_tokenizer  0.959140  0.959131"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code Cell 3.4.3.1 ###\n",
    "\n",
    "result_df = pd.DataFrame(results2)\n",
    "\n",
    "result_df[['model','tokenizer','accuracy','f1_score']] \\\n",
    "    .sort_values(by=['accuracy'], ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากผลลัพธ์ของเซลล์โค้ดที่ 3.4.3.1 การจำแนกข้อความหัวข้อต่อกับเนื้อความของบทความ (contents_with_title) ที่ทำการ preprocess โดยใช้ snowball_stem_tokenizer ด้วย RandomForestClassifier ให้ประสิทธิภาพดีที่สุด ซึ่งมี accuracy อยู่ที่ 97.6344% และ f1 score อยู่ที่ 0.976341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.976344</td>\n",
       "      <td>0.976341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.976344</td>\n",
       "      <td>0.976336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.974194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.974194</td>\n",
       "      <td>0.974155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.972004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.972043</td>\n",
       "      <td>0.972012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tokenizer                   model  accuracy  f1_score\n",
       "0      snowball_stem_tokenizer  RandomForestClassifier  0.976344  0.976341\n",
       "1     lancaster_stem_tokenizer      LogisticRegression  0.976344  0.976336\n",
       "2        porter_stem_tokenizer      LogisticRegression  0.974194  0.974194\n",
       "3               word_tokenizer      LogisticRegression  0.974194  0.974155\n",
       "4      wordnet_lemma_tokenizer  RandomForestClassifier  0.972043  0.972004\n",
       "5  wordnet_lemma_pos_tokenizer      LogisticRegression  0.972043  0.972012"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code Cell 3.4.3.2 ###\n",
    "\n",
    "result_df[['tokenizer','model','accuracy','f1_score']] \\\n",
    "    .sort_values(by=['accuracy'], ascending=False) \\\n",
    "    .groupby('tokenizer') \\\n",
    "    .head(1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากผลลัพธ์ของเซลล์โค้ดที่ 3.4.3.2 พบว่าจำแนกข้อความหัวข้อต่อกับเนื้อความของบทความ (contents_with_title) ที่ทำการ preprocess โดยใช้ snowball_stem_tokenizer, lancaster_stem_tokenizer, porter_stem_tokenizer, word_tokenizer, wordnet_lemma_tokenizer, wordnet_lemma_pos_tokenizer ได้ดีที่สุดตามลำดับ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Try Transforming with LDA 1\n",
    "\n",
    "นำข้อมูลเนื้อความของบทความ (contents_X) มาทดลองลดมิติข้อมูลลงด้วย LDA และทำการจำแนกแบบเดียวกันกับข้อที่ 3.3 เพื่อดูว่าจะทำให้การจำแนกมีประสิทธิภาพดีขึ้นหรือไม่"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.1 ลดมิติข้อมูลเนื้อความของบทความ (contents_X) ด้วย LDA ให้เหลือ 2 features ตามเซลล์โค้ดที่ 3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data tokenized by word_tokenizer\n",
      "Transforming data tokenized by porter_stem_tokenizer\n",
      "Transforming data tokenized by snowball_stem_tokenizer\n",
      "Transforming data tokenized by lancaster_stem_tokenizer\n",
      "Transforming data tokenized by wordnet_lemma_tokenizer\n",
      "Transforming data tokenized by wordnet_lemma_pos_tokenizer\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 3.5.1 ###\n",
    "\n",
    "contents_X_lda = dict()\n",
    "\n",
    "for tokenizer_name, X in contents_X.items():\n",
    "  print(f'Transforming data tokenized by {tokenizer_name}')\n",
    "  lda = LDA(n_components=2)\n",
    "  contents_X_lda[tokenizer_name] = lda.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.2 กำหนด model และช่วงของค่า parameter ที่ต้องการทำ tuning ตามเซลล์โค้ดที่ 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.5.2 ###\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\"n_neighbors\": list(range(3, 16, 2))}\n",
    "    },\n",
    "    {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            \"C\": np.arange(0.1, 1.1, 0.1)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": list(range(25, 201, 25)),\n",
    "            \"min_samples_split\": list(range(5, 26, 5)),\n",
    "            \"random_state\": [28]\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.3 ทำการ parameter tuning หา parameter ที่ให้ค่า accuracy ที่ดีที่สุดด้วย 5-fold cross validation (GridSearchCV) ของคู่ model กับข้อมูลที่ผ่านการเตรียมด้วย tokenizer แบบต่างๆ จากนั้นนำมาทดสอบกับ test set เพื่อประเมินประสิทธิภาพ model ตามเซลล์โค้ดที่ 3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs tokenized by word_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 1.0000, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 1.0000, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9989, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by porter_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 1.0000, params {'n_neighbors': 7}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 1.0000, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9989, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by snowball_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9989, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 1.0000, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9989, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by lancaster_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 1.0000, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 1.0000, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9989, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by wordnet_lemma_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 1.0000, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 1.0000, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9989, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by wordnet_lemma_pos_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9989, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 1.0000, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9989, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 3.5.3 ###\n",
    "\n",
    "results3 = {\n",
    "    \"model\": [],\n",
    "    \"tokenizer\": [],\n",
    "    \"best_tuning_params\": [],\n",
    "    \"best_tuning_acc\": [],\n",
    "    \"accuracy\":[],\n",
    "    \"f1_score\":[],\n",
    "}\n",
    "\n",
    "y = targets\n",
    "\n",
    "for tokenizer_name, X in contents_X_lda.items():\n",
    "    print(f'Docs tokenized by {tokenizer_name}')\n",
    "\n",
    "    # Stratified train-test split with test size of 33%\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "        random_state=50, test_size=.33, stratify=y) \n",
    "\n",
    "    for model in models:\n",
    "        print(f\"- Tuning {type(model['model']).__name__}:\")\n",
    "\n",
    "        # Search for best set of parameters, using 5-fold cross validation\n",
    "        clf = GridSearchCV(model['model'], model['params'], cv=5, n_jobs=-1, verbose=1)\n",
    "        result = clf.fit(X_train, y_train)\n",
    "        results3['model'].append(type(model['model']).__name__)\n",
    "        results3['tokenizer'].append(tokenizer_name)\n",
    "        results3['best_tuning_params'].append(result.best_params_)\n",
    "        results3['best_tuning_acc'].append(result.best_score_)\n",
    "        print(f'Best tuning: acc {result.best_score_:.4f}, params {result.best_params_}')\n",
    "\n",
    "        # Test model with best set of parameters from GridSearchCV\n",
    "        model = model['model']\n",
    "        model.set_params(**result.best_params_)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results3['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        results3['f1_score'].append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5.4 แสดงผลลัพธ์จากข้อ 3.5.3 ตามเซลล์โค้ดที่ 3.5.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.997849</td>\n",
       "      <td>0.997850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.997849</td>\n",
       "      <td>0.997849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model                    tokenizer  accuracy  f1_score\n",
       "6     KNeighborsClassifier      snowball_stem_tokenizer  0.997849  0.997850\n",
       "15    KNeighborsClassifier  wordnet_lemma_pos_tokenizer  0.997849  0.997849\n",
       "0     KNeighborsClassifier               word_tokenizer  0.995699  0.995704\n",
       "1       LogisticRegression               word_tokenizer  0.995699  0.995704\n",
       "2   RandomForestClassifier               word_tokenizer  0.995699  0.995704\n",
       "4       LogisticRegression        porter_stem_tokenizer  0.995699  0.995704\n",
       "5   RandomForestClassifier        porter_stem_tokenizer  0.995699  0.995704\n",
       "7       LogisticRegression      snowball_stem_tokenizer  0.995699  0.995704\n",
       "8   RandomForestClassifier      snowball_stem_tokenizer  0.995699  0.995704\n",
       "9     KNeighborsClassifier     lancaster_stem_tokenizer  0.995699  0.995704\n",
       "10      LogisticRegression     lancaster_stem_tokenizer  0.995699  0.995704\n",
       "11  RandomForestClassifier     lancaster_stem_tokenizer  0.995699  0.995704\n",
       "12    KNeighborsClassifier      wordnet_lemma_tokenizer  0.995699  0.995704\n",
       "13      LogisticRegression      wordnet_lemma_tokenizer  0.995699  0.995704\n",
       "14  RandomForestClassifier      wordnet_lemma_tokenizer  0.995699  0.995704\n",
       "16      LogisticRegression  wordnet_lemma_pos_tokenizer  0.995699  0.995704\n",
       "17  RandomForestClassifier  wordnet_lemma_pos_tokenizer  0.995699  0.995704\n",
       "3     KNeighborsClassifier        porter_stem_tokenizer  0.995699  0.995698"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code Cell 3.5.4 ###\n",
    "\n",
    "result_df = pd.DataFrame(results3)\n",
    "\n",
    "result_df[['model','tokenizer','accuracy','f1_score']] \\\n",
    "    .sort_values(by=['accuracy','f1_score'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากผลลัพธ์ของเซลล์โค้ดที่ 3.5.4 การจำแนกข้อความเนื้อความของบทความ (contents) ที่ทำการ preprocess โดยใช้ snowball_stem_tokenizer แล้วทำการลดมิติข้อมูล ด้วย KNeighborsClassifier ให้ประสิทธิภาพดีที่สุด ซึ่งมี accuracy อยู่ที่ 99.7849% และ f1 score อยู่ที่ 0.997850\n",
    "\n",
    "ซึ่งสรุปได้ว่าการลดมิติข้อมูลด้วย LDA ทำให้การจำแนกมีประสิทธิภาพดีขึ้น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Try Transforming with LDA 2\n",
    "\n",
    "นำข้อมูลหัวข้อต่อกับเนื้อความของบทความ (contents_with_title_X) มาทดลองลดมิติข้อมูลลงด้วย LDA และทำการจำแนกแบบเดียวกันกับข้อที่ 3.4 เพื่อดูว่าจะทำให้การจำแนกมีประสิทธิภาพดีขึ้นหรือไม่"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6.1 ลดมิติข้อมูลเนื้อความของบทความ (contents_X) ด้วย LDA ให้เหลือ 2 features ตามเซลล์โค้ดที่ 3.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data tokenized by word_tokenizer\n",
      "Transforming data tokenized by porter_stem_tokenizer\n",
      "Transforming data tokenized by snowball_stem_tokenizer\n",
      "Transforming data tokenized by lancaster_stem_tokenizer\n",
      "Transforming data tokenized by wordnet_lemma_tokenizer\n",
      "Transforming data tokenized by wordnet_lemma_pos_tokenizer\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 3.6.1 ###\n",
    "\n",
    "contents_with_title_X_lda = dict()\n",
    "\n",
    "for tokenizer_name, X in contents_with_title_X.items():\n",
    "  print(f'Transforming data tokenized by {tokenizer_name}')\n",
    "  lda = LDA(n_components=2)\n",
    "  contents_with_title_X_lda[tokenizer_name] = lda.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6.2 กำหนด model และช่วงของค่า parameter ที่ต้องการทำ tuning ตามเซลล์โค้ดที่ 3.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.6.2 ###\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\"n_neighbors\": list(range(3, 16, 2))}\n",
    "    },\n",
    "    {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            \"C\": np.arange(0.1, 1.1, 0.1)\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": list(range(25, 201, 25)),\n",
    "            \"min_samples_split\": list(range(5, 26, 5)),\n",
    "            \"random_state\": [28]\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6.3 ทำการ parameter tuning หา parameter ที่ให้ค่า accuracy ที่ดีที่สุดด้วย 5-fold cross validation (GridSearchCV) ของคู่ model กับข้อมูลที่ผ่านการเตรียมด้วย tokenizer แบบต่างๆ จากนั้นนำมาทดสอบกับ test set เพื่อประเมินประสิทธิภาพ model ตามเซลล์โค้ดที่ 3.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs tokenized by word_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9989, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9989, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9989, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by porter_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9989, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 1.0000, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 1.0000, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by snowball_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9989, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 1.0000, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 1.0000, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by lancaster_stem_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9989, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 1.0000, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9989, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by wordnet_lemma_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9989, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 0.9989, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9979, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n",
      "Docs tokenized by wordnet_lemma_pos_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.9989, params {'n_neighbors': 3}\n",
      "- Tuning LogisticRegression:\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best tuning: acc 1.0000, params {'C': 0.1, 'solver': 'newton-cg'}\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best tuning: acc 0.9989, params {'min_samples_split': 5, 'n_estimators': 25, 'random_state': 28}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 3.6.3 ###\n",
    "\n",
    "results4 = {\n",
    "    \"model\": [],\n",
    "    \"tokenizer\": [],\n",
    "    \"best_tuning_params\": [],\n",
    "    \"best_tuning_acc\": [],\n",
    "    \"accuracy\":[],\n",
    "    \"f1_score\":[],\n",
    "}\n",
    "\n",
    "y = targets\n",
    "\n",
    "for tokenizer_name, X in contents_with_title_X_lda.items():\n",
    "    print(f'Docs tokenized by {tokenizer_name}')\n",
    "\n",
    "    # Stratified train-test split with test size of 33%\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "        random_state=50, test_size=.33, stratify=y) \n",
    "\n",
    "    for model in models:\n",
    "        print(f\"- Tuning {type(model['model']).__name__}:\")\n",
    "\n",
    "        # Search for best set of parameters, using 5-fold cross validation\n",
    "        clf = GridSearchCV(model['model'], model['params'], cv=5, n_jobs=-1, verbose=1)\n",
    "        result = clf.fit(X_train, y_train)\n",
    "        results4['model'].append(type(model['model']).__name__)\n",
    "        results4['tokenizer'].append(tokenizer_name)\n",
    "        results4['best_tuning_params'].append(result.best_params_)\n",
    "        results4['best_tuning_acc'].append(result.best_score_)\n",
    "        print(f'Best tuning: acc {result.best_score_:.4f}, params {result.best_params_}')\n",
    "\n",
    "        # Test model with best set of parameters from GridSearchCV\n",
    "        model = model['model']\n",
    "        model.set_params(**result.best_params_)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results4['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        results4['f1_score'].append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6.4 แสดงผลลัพธ์จากข้อ 3.6.3 ตามเซลล์โค้ดที่ 3.6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.997849</td>\n",
       "      <td>0.997848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.995699</td>\n",
       "      <td>0.995699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>word_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>porter_stem_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>snowball_stem_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>lancaster_stem_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>wordnet_lemma_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>wordnet_lemma_pos_tokenizer</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.993553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model                    tokenizer  accuracy  f1_score\n",
       "12    KNeighborsClassifier      wordnet_lemma_tokenizer  0.997849  0.997848\n",
       "15    KNeighborsClassifier  wordnet_lemma_pos_tokenizer  0.995699  0.995699\n",
       "2   RandomForestClassifier               word_tokenizer  0.995699  0.995699\n",
       "14  RandomForestClassifier      wordnet_lemma_tokenizer  0.995699  0.995699\n",
       "0     KNeighborsClassifier               word_tokenizer  0.993548  0.993553\n",
       "1       LogisticRegression               word_tokenizer  0.993548  0.993553\n",
       "3     KNeighborsClassifier        porter_stem_tokenizer  0.993548  0.993553\n",
       "4       LogisticRegression        porter_stem_tokenizer  0.993548  0.993553\n",
       "5   RandomForestClassifier        porter_stem_tokenizer  0.993548  0.993553\n",
       "6     KNeighborsClassifier      snowball_stem_tokenizer  0.993548  0.993553\n",
       "7       LogisticRegression      snowball_stem_tokenizer  0.993548  0.993553\n",
       "8   RandomForestClassifier      snowball_stem_tokenizer  0.993548  0.993553\n",
       "9     KNeighborsClassifier     lancaster_stem_tokenizer  0.993548  0.993553\n",
       "10      LogisticRegression     lancaster_stem_tokenizer  0.993548  0.993553\n",
       "11  RandomForestClassifier     lancaster_stem_tokenizer  0.993548  0.993553\n",
       "13      LogisticRegression      wordnet_lemma_tokenizer  0.993548  0.993553\n",
       "16      LogisticRegression  wordnet_lemma_pos_tokenizer  0.993548  0.993553\n",
       "17  RandomForestClassifier  wordnet_lemma_pos_tokenizer  0.993548  0.993553"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code Cell 3.6.4 ###\n",
    "\n",
    "result_df = pd.DataFrame(results4)\n",
    "\n",
    "result_df[['model','tokenizer','accuracy','f1_score']] \\\n",
    "    .sort_values(by=['accuracy','f1_score'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากผลลัพธ์ของเซลล์โค้ดที่ 3.6.4 การจำแนกข้อความเนื้อความของบทความ (contents) ที่ทำการ preprocess โดยใช้ wordnet_lemma_tokenizer แล้วทำการลดมิติข้อมูล ด้วย KNeighborsClassifier ให้ประสิทธิภาพดีที่สุด ซึ่งมี accuracy อยู่ที่ 99.7849% และ f1 score อยู่ที่ 0.997848\n",
    "\n",
    "ซึ่งสรุปได้ว่าการลดมิติข้อมูลด้วย LDA ทำให้การจำแนกมีประสิทธิภาพดีขึ้น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Summary goes here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2fe7d652243bcdb6cde2e69617dc664bf14361ea2de14b139d2fec55ec2ae7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
