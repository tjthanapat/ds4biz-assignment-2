{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Task 2\n",
    "\n",
    "โดย\n",
    "\n",
    "ตุลธร วงศ์ชัย รหัสนักศึกษา 63070224\n",
    "\n",
    "ธนภัทร ธีรรัตตัญญู รหัสนักศึกษา 63070227\n",
    "\n",
    "> **README**:\n",
    "> - 'Run all' is not recommended. \n",
    "> - Section 1 must be run before any other section.\n",
    "> - Read and follow instruction of each section carefully if it is provided.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Init Project\n",
    "\n",
    "> **Instruction**: \n",
    "> - Set file path of `traffy_fondue_tickets.csv` in section 1.2 before running this section.\n",
    "> - Although `attacut` is not imported directly, it is required installation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>running_id</th>\n",
       "      <th>issues</th>\n",
       "      <th>การเดินทาง</th>\n",
       "      <th>กีดขวาง</th>\n",
       "      <th>คนจรจัด</th>\n",
       "      <th>คลอง</th>\n",
       "      <th>ความปลอดภัย</th>\n",
       "      <th>ความสะอาด</th>\n",
       "      <th>จราจร</th>\n",
       "      <th>ต้นไม้</th>\n",
       "      <th>...</th>\n",
       "      <th>ป้ายจราจร</th>\n",
       "      <th>ร้องเรียน</th>\n",
       "      <th>สอบถาม</th>\n",
       "      <th>สะพาน</th>\n",
       "      <th>สัตว์จรจัด</th>\n",
       "      <th>สายไฟ</th>\n",
       "      <th>ห้องน้ำ</th>\n",
       "      <th>เสนอแนะ</th>\n",
       "      <th>เสียงรบกวน</th>\n",
       "      <th>แสงสว่าง</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ทำเรื่องรับสมุดทหารกองหนุน หลังจบแล้วมีให้เสีย...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ท่อประปาแตกหน้าบ้าน บ้านเลขที่126 ซ.สุขสวัสดิ์...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ขอแจ้งเหตุถนนเป็นหลุม ที่บริเวณหน้าหมู่บ้าน ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>โครงการเพอร์เฟคเพลสพระราม9กรุงเทพกรีฑา ใช้รถน้...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>เคยแจ้งทาง เขตแล้วให้ช่วยเก็บตามบ้าน แต่ละซอย ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>10105</td>\n",
       "      <td>น้ำท่วมขังสูง มาตั้งแต่วันจันทร์ที่ 1 สค จนถึง...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>10106</td>\n",
       "      <td>สะพานลอยแถวถนนกัลปพฤกษ์ก่อนถึงแยกราชพฤกษ์ ทำไม...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8586</th>\n",
       "      <td>10107</td>\n",
       "      <td>มีผู้ค้ารายใหม่มาตั้งร้านบนถนนในจุดที่เจ้าหน้า...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8587</th>\n",
       "      <td>10108</td>\n",
       "      <td>1.ปัญหาการจราจรในซอยบางแค-บางบอน แยกพัฒนาการ เ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8588</th>\n",
       "      <td>10109</td>\n",
       "      <td>สะพานในสวนน้ำบึงกุ่ม\\n สวนเสรีไทย ที่ใช้ข้ามไป...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8589 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      running_id                                             issues  \\\n",
       "0              1  ทำเรื่องรับสมุดทหารกองหนุน หลังจบแล้วมีให้เสีย...   \n",
       "1              2  ท่อประปาแตกหน้าบ้าน บ้านเลขที่126 ซ.สุขสวัสดิ์...   \n",
       "2              3  ขอแจ้งเหตุถนนเป็นหลุม ที่บริเวณหน้าหมู่บ้าน ca...   \n",
       "3              4  โครงการเพอร์เฟคเพลสพระราม9กรุงเทพกรีฑา ใช้รถน้...   \n",
       "4              5  เคยแจ้งทาง เขตแล้วให้ช่วยเก็บตามบ้าน แต่ละซอย ...   \n",
       "...          ...                                                ...   \n",
       "8584       10105  น้ำท่วมขังสูง มาตั้งแต่วันจันทร์ที่ 1 สค จนถึง...   \n",
       "8585       10106  สะพานลอยแถวถนนกัลปพฤกษ์ก่อนถึงแยกราชพฤกษ์ ทำไม...   \n",
       "8586       10107  มีผู้ค้ารายใหม่มาตั้งร้านบนถนนในจุดที่เจ้าหน้า...   \n",
       "8587       10108  1.ปัญหาการจราจรในซอยบางแค-บางบอน แยกพัฒนาการ เ...   \n",
       "8588       10109  สะพานในสวนน้ำบึงกุ่ม\\n สวนเสรีไทย ที่ใช้ข้ามไป...   \n",
       "\n",
       "      การเดินทาง  กีดขวาง  คนจรจัด  คลอง  ความปลอดภัย  ความสะอาด  จราจร  \\\n",
       "0              0        0        0     0            0          0      0   \n",
       "1              0        0        0     0            0          0      0   \n",
       "2              0        0        0     0            1          0      0   \n",
       "3              0        0        0     0            0          1      0   \n",
       "4              0        0        0     0            0          0      0   \n",
       "...          ...      ...      ...   ...          ...        ...    ...   \n",
       "8584           0        0        0     0            0          0      0   \n",
       "8585           0        0        0     0            0          0      1   \n",
       "8586           0        1        0     0            0          0      0   \n",
       "8587           0        0        0     0            0          0      1   \n",
       "8588           0        0        0     0            0          0      0   \n",
       "\n",
       "      ต้นไม้  ...  ป้ายจราจร  ร้องเรียน  สอบถาม  สะพาน  สัตว์จรจัด  สายไฟ  \\\n",
       "0          0  ...          0          1       0      0           0      0   \n",
       "1          0  ...          0          1       0      0           0      0   \n",
       "2          0  ...          0          0       0      0           0      0   \n",
       "3          0  ...          0          1       0      0           0      0   \n",
       "4          0  ...          0          1       0      0           0      0   \n",
       "...      ...  ...        ...        ...     ...    ...         ...    ...   \n",
       "8584       0  ...          0          0       0      0           0      0   \n",
       "8585       0  ...          0          1       0      0           0      0   \n",
       "8586       0  ...          0          1       0      0           0      0   \n",
       "8587       0  ...          0          1       0      0           0      0   \n",
       "8588       0  ...          0          1       0      1           0      0   \n",
       "\n",
       "      ห้องน้ำ  เสนอแนะ  เสียงรบกวน  แสงสว่าง  \n",
       "0           0        0           0         0  \n",
       "1           0        0           0         0  \n",
       "2           0        0           0         0  \n",
       "3           0        0           1         0  \n",
       "4           0        0           0         0  \n",
       "...       ...      ...         ...       ...  \n",
       "8584        0        0           0         0  \n",
       "8585        0        0           0         0  \n",
       "8586        0        0           0         0  \n",
       "8587        0        0           0         0  \n",
       "8588        0        0           0         0  \n",
       "\n",
       "[8589 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(f'{os.getcwd()}/traffy_fondue_tickets.csv')\n",
    "raw_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define Word Tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1 กำหนดเซ็ตของ stop words ตามเซลล์โค้ดที่ 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 2.1.1 ###\n",
    "\n",
    "STOP_WORDS = thai_stopwords()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 สร้าง tokenizer โดยนำเอา word tokenizer จาก PyThaiNLP มาเพิ่มการกรองให้เหลือแต่คำที่มีตัวอักษรภาษาไทยและภาษาอังกฤษโดยใช้ regular expression และกรองเอาคำ stop words ออก ตามเซลล์โค้ดที่ 2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 2.1.2 ###\n",
    "\n",
    "def thai_word_tokenizer(text: str, engine: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize given text using PyThaiNLP's word tokenizer\n",
    "    with filtering stop words and matching only word containing \n",
    "    Thai letters and English letters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "      Text to be tokenized\n",
    "    engine : str\n",
    "      Name of engine available in PyThaiNLP\n",
    "      e.g. newmm, longest, attacut, deepcut \n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "      List of tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text, engine=engine)\n",
    "\n",
    "    tokens_to_return = list()\n",
    "    for token in tokens:\n",
    "        if (re.match(r\"([\\u0E00-\\u0E7Fa-zA-Z])+\", token) and (token not in STOP_WORDS)):\n",
    "            tokens_to_return.append(token)\n",
    "    return tokens_to_return\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.3 สร้าง tokenizer จากตัว thai_word_tokenizer โดยกำหนด engine ดังนี้\n",
    "\n",
    "- newmm ที่เป็น dictionary-based\n",
    "- attacut ที่เป็น learning-based\n",
    "\n",
    "ตามเซลล์โค้ดที่ 2.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 2.1.3 ###\n",
    "\n",
    "def newmm_tokenizer(text:str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize given text using custom Thai word tokenizer\n",
    "    with newmm engine\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "      Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "      List of tokens\n",
    "    \"\"\"\n",
    "    return thai_word_tokenizer(text, engine='newmm')\n",
    "\n",
    "def attacut_tokenizer(text:str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize given text using custom Thai word tokenizer\n",
    "    with attacut engine\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "      Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "      List of tokens\n",
    "    \"\"\"\n",
    "    return thai_word_tokenizer(text, engine='attacut')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.4 ทดสอบ tokenizer ที่สร้างขึ้นในข้อ 2.1.3 ตามเซลล์โค้ดที่ 2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "สำหรับวัคซีนไฟเซอร์ฝาส้ม เป็นวัคซีนสูตรสำหรับเด็ก (paediatric formulation dose) \n",
      "ซึ่งต่างจากวัคซีนสูตรผู้ใหญ่ที่เป็นฝาสีม่วง ข้อมูล ณ วันที่ 21 ม.ค. กรมควบคุมโรคระบุว่า \n",
      "ได้รับแจ้งจากบริษัทผู้ผลิตว่าส่งมอบได้ในไตรมาสแรกปีนี้ 3.5 ล้านโดส \n",
      "ทั้งนี้ ครม.อนุมัติงบประมาณจัดซื้อไว้ 10 ล้านโดส\n",
      "Tokenized by newmm tokenizer:\n",
      "['สำหรับ', 'วัคซีน', 'ไฟ', 'เซอร์', 'ฝา', 'ส้ม', 'วัคซีน', 'สูตร', 'สำหรับ', 'เด็ก', 'paediatric', 'formulation', 'dose', 'วัคซีน', 'สูตร', 'ผู้ใหญ่', 'ฝา', 'สีม่วง', 'ข้อมูล', 'วันที่', 'ม.ค.', 'กรม', 'ควบคุม', 'โรค', 'ระบุ', 'แจ้ง', 'บริษัทผู้ผลิต', 'ส่งมอบ', 'ไตรมาส', 'ปี', 'ล้าน', 'โดส', 'ครม.', 'อนุมัติ', 'งบประมาณ', 'จัดซื้อ', 'ล้าน', 'โดส']\n",
      "Tokenized by attacut tokenizer:\n",
      "['สำหรับ', 'วัคซีน', 'ไฟเซอร์', 'ฝา', 'ส้ม', 'วัคซีน', 'สูตร', 'สำหรับ', 'เด็ก', 'paediatric', 'formulation', 'dose', 'วัคซีน', 'สูตร', 'ฝา', 'สี', 'ม่วง', 'ข้อมูล', 'ม.ค.', 'กรมควบคุมโรค', 'ระบุ', 'แจ้ง', 'บริษัท', 'ผลิต', 'ส่งมอบ', 'ไตรมาส', 'ปี', 'ล้าน', 'โดส', 'ครม.', 'อนุมัติ', 'งบ', 'จัดซื้อ', 'ล้าน', 'โดส']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"สำหรับวัคซีนไฟเซอร์ฝาส้ม เป็นวัคซีนสูตรสำหรับเด็ก (paediatric formulation dose) \n",
    "ซึ่งต่างจากวัคซีนสูตรผู้ใหญ่ที่เป็นฝาสีม่วง ข้อมูล ณ วันที่ 21 ม.ค. กรมควบคุมโรคระบุว่า \n",
    "ได้รับแจ้งจากบริษัทผู้ผลิตว่าส่งมอบได้ในไตรมาสแรกปีนี้ 3.5 ล้านโดส \n",
    "ทั้งนี้ ครม.อนุมัติงบประมาณจัดซื้อไว้ 10 ล้านโดส\"\"\"\n",
    "\n",
    "print('Original Text:')\n",
    "print(text)\n",
    "print('Tokenized by newmm tokenizer:')\n",
    "print(newmm_tokenizer(text))\n",
    "print('Tokenized by attacut tokenizer:')\n",
    "print(attacut_tokenizer(text))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Vectorizing Docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 ทำการ tokenize ข้อความด้วย tokenizer จากข้อ 2.1 จากนั้นนำไปเข้า TFIDF vectorizer เพื่อทำการ term weighting โดยที่กำหนด min_df อยู่ที่ 30 และ max_df อยู่ที่ 0.95 ตามเซลล์โค้ดที่ 2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing docs with newmm_tokenizer\n",
      "vectorizing docs with attacut_tokenizer\n"
     ]
    }
   ],
   "source": [
    "docs = raw_data['issues']\n",
    "X_tfidf = dict()\n",
    "\n",
    "tokenizers = [newmm_tokenizer, attacut_tokenizer]\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "    print(f'vectorizing docs with {tokenizer.__name__}')\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenizer, min_df=30, max_df=.95)\n",
    "    term_weighted = vectorizer.fit_transform(docs)\n",
    "    X_tfidf[tokenizer.__name__] = {\n",
    "        'data': term_weighted.toarray(),\n",
    "        'vectorizer': vectorizer\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification 1\n",
    "\n",
    "นำข้อมูลที่ได้เตรียมไว้จากข้อ 2 (X_tfidf) มาทำการทดลองกับโมเดล KNN และ Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 กำหนด model และช่วงของค่า parameter ที่ต้องการทำ tuning ตามเซลล์โค้ดที่ 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 3.1 ###\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\"n_neighbors\": list(range(3, 16, 2))}\n",
    "    },\n",
    "    {\n",
    "        \"model\": RandomForestClassifier(random_state=28),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": list(range(25, 101, 25)),\n",
    "            \"min_samples_split\": list(range(5, 26, 5)),\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 ทำการ parameter tuning หา parameter ที่ให้ค่า accuracy ที่ดีที่สุดด้วย 5-fold cross validation (GridSearchCV) ของคู่ model กับข้อมูลที่ผ่านการเตรียมด้วย tokenizer แบบต่างๆ จากนั้นนำมาทดสอบกับ test set เพื่อประเมินประสิทธิภาพ model ตามเซลล์โค้ดที่ 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs tokenized by newmm_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.1945, params {'n_neighbors': 13}\n",
      "Testing accuracy: 0.1954, hamming loss: 0.0580\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best tuning: acc 0.3116, params {'min_samples_split': 5, 'n_estimators': 100}\n",
      "Testing accuracy: 0.3266, hamming loss: 0.0461\n",
      "----------\n",
      "Docs tokenized by attacut_tokenizer\n",
      "- Tuning KNeighborsClassifier:\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "Best tuning: acc 0.2452, params {'n_neighbors': 15}\n",
      "Testing accuracy: 0.2519, hamming loss: 0.0516\n",
      "- Tuning RandomForestClassifier:\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best tuning: acc 0.3222, params {'min_samples_split': 5, 'n_estimators': 75}\n",
      "Testing accuracy: 0.3362, hamming loss: 0.0445\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 3.2 ###\n",
    "\n",
    "results1 = list()\n",
    "\n",
    "y = raw_data.iloc[:, 2:]\n",
    "\n",
    "for tokenizer_name, X in X_tfidf.items():\n",
    "    print(f'Docs tokenized by {tokenizer_name}')\n",
    "\n",
    "    # Train-test split with test size of 33%\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X['data'], y,\n",
    "        random_state=50, test_size=.33)\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"- Tuning {type(model['model']).__name__}:\")\n",
    "\n",
    "        # Search for best set of parameters, using 5-fold cross validation\n",
    "        clf = GridSearchCV(\n",
    "            model['model'], model['params'], cv=5, n_jobs=-1, verbose=1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(\n",
    "            f'Best tuning: acc {clf.best_score_:.4f}, params {clf.best_params_}')\n",
    "\n",
    "        # Test model with best set of parameters from GridSearchCV\n",
    "        y_pred = clf.best_estimator_.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        test_hamming_loss = hamming_loss(y_test, y_pred)\n",
    "        print(f'Testing accuracy: {test_accuracy:.4f}, ' +\n",
    "              f'hamming loss: {test_hamming_loss:.4f}')\n",
    "\n",
    "        result = {\n",
    "            'model': type(model['model']).__name__,\n",
    "            'tokenizer': tokenizer_name,\n",
    "            'best_tuning_params': clf.best_params_,\n",
    "            'best_tuning_acc': clf.best_score_,\n",
    "            'best_estimator': clf.best_estimator_,\n",
    "            'accuracy': test_accuracy,\n",
    "            'hamming_loss': test_hamming_loss}\n",
    "\n",
    "        results1.append(result)\n",
    "\n",
    "    print('-'*10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 แสดงผลลัพธ์จากข้อ 3.2 ตามเซลล์โค้ดที่ 3.3.1 - 3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>tokenizer</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>hamming_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>attacut_tokenizer</td>\n",
       "      <td>0.336155</td>\n",
       "      <td>0.044490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>newmm_tokenizer</td>\n",
       "      <td>0.326631</td>\n",
       "      <td>0.046101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>attacut_tokenizer</td>\n",
       "      <td>0.251852</td>\n",
       "      <td>0.051591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>newmm_tokenizer</td>\n",
       "      <td>0.195414</td>\n",
       "      <td>0.058002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model          tokenizer  accuracy  hamming_loss\n",
       "0  RandomForestClassifier  attacut_tokenizer  0.336155      0.044490\n",
       "1  RandomForestClassifier    newmm_tokenizer  0.326631      0.046101\n",
       "2    KNeighborsClassifier  attacut_tokenizer  0.251852      0.051591\n",
       "3    KNeighborsClassifier    newmm_tokenizer  0.195414      0.058002"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code Cell 3.3.1 ###\n",
    "\n",
    "result_df = pd.DataFrame(results1)\n",
    "\n",
    "result_df[['model', 'tokenizer', 'accuracy', 'hamming_loss']] \\\n",
    "    .sort_values(by=['accuracy', 'hamming_loss'], ascending=False) \\\n",
    "    .reset_index(drop=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากผลลัพธ์ของเซลล์โค้ดที่ 3.3 พบว่าผลการทดลองที่ดีที่สุด คือ การเตรียมข้อมูลด้วย attacut_tokenizer และใช้ RandomForestClassifier ที่กำหนดให้ n_estimators = 75 และ min_samples_split = 5 ซึ่งมีค่า accuracy = 0.336155 และ hamming loss = 0.044490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'RandomForestClassifier',\n",
       " 'tokenizer': 'attacut_tokenizer',\n",
       " 'best_tuning_params': {'min_samples_split': 5, 'n_estimators': 75},\n",
       " 'best_tuning_acc': 0.3222128206096778,\n",
       " 'best_estimator': RandomForestClassifier(min_samples_split=5, n_estimators=75, random_state=28),\n",
       " 'accuracy': 0.3361552028218695,\n",
       " 'hamming_loss': 0.04449045318610536}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code Cell 3.3.2 ###\n",
    "\n",
    "# Best model\n",
    "results1[3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification 2 (proposed idea)\n",
    "\n",
    "นำ model ที่ดีที่สุดจากการทดลองในข้อ 3 (RandomForestClassifier ที่กำหนดให้ n_estimators = 75 และ min_samples_split = 5) มาปรับเพิ่ม prop ของผลลัพธ์ของแต่ละ label class อิงจาก frequent_terms ของ class นั้นๆ\n",
    "\n",
    "กำหนดให้ frequent_term คือ term ที่มี document frequency สูง โดยเทียบกับ document ที่อยู่ใน label class หนึ่งๆ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 หา frequent_term ของแต่ละ label class โดยกำหนดให้ term ที่จะเป็น frequent_term ได้ต้องมี df >= 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "การเดินทาง: []\n",
      "กีดขวาง: []\n",
      "คนจรจัด: ['คน']\n",
      "คลอง: ['คลอง']\n",
      "ความปลอดภัย: []\n",
      "ความสะอาด: []\n",
      "จราจร: ['รถ']\n",
      "ต้นไม้: ['ต้น', 'ไม้']\n",
      "ถนน: ['ถนน']\n",
      "ทางเท้า: []\n",
      "ท่อระบายน้ำ: ['ท่อ', 'น้ำ']\n",
      "น้ำท่วม: ['ท่วม', 'น้ำ']\n",
      "ป้าย: ['ป้าย']\n",
      "ป้ายจราจร: ['ป้าย']\n",
      "ร้องเรียน: []\n",
      "สอบถาม: []\n",
      "สะพาน: ['สะพาน']\n",
      "สัตว์จรจัด: []\n",
      "สายไฟ: ['สาย']\n",
      "ห้องน้ำ: ['ห้องน้ำ']\n",
      "เสนอแนะ: []\n",
      "เสียงรบกวน: ['เสียง']\n",
      "แสงสว่าง: ['ไฟ']\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 4.1 ###\n",
    "\n",
    "frequent_terms_in_classes = list()\n",
    "\n",
    "for i in range(23):\n",
    "    print(f'{raw_data.columns[2+i]}', end=': ')\n",
    "    docs = raw_data['issues'][raw_data.iloc[:,2+i]==1]\n",
    "\n",
    "    # Find frequent terms\n",
    "    try:\n",
    "      count_vectorizer = CountVectorizer(tokenizer=attacut_tokenizer, \n",
    "          token_pattern=None, min_df=.75)\n",
    "      count_vectorizer.fit(docs)\n",
    "      frequent_terms = list(count_vectorizer.vocabulary_.keys())\n",
    "    except:\n",
    "      frequent_terms = list()\n",
    "    \n",
    "    print(frequent_terms)\n",
    "    frequent_terms_in_classes.append(frequent_terms)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากผลลัพธ์เซลล์โค้ดที่ 4.1 แสดงผลลัพธ์ของ frequent_terms ของแต่ละ label class เช่น frequent_terms ของ label class 'คนจรจัด' ได้แก่ ['คน']; frequent_terms ของ label class 'ท่อระบายน้ำ' ได้แก่ ['ท่อ', 'น้ำ']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 แปลง term เป็น index ของ term ใน feature vector ตามเซลล์โค้ดที่ 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 4.2 ###\n",
    "\n",
    "vectorizer = X_tfidf['attacut_tokenizer']['vectorizer']\n",
    "\n",
    "frequent_terms_in_classes_index = list()\n",
    "\n",
    "for label_class in frequent_terms_in_classes:\n",
    "    frequent_terms_index = list()\n",
    "    for frequent_term in label_class:\n",
    "        frequent_terms_index.append(vectorizer.vocabulary_[frequent_term])\n",
    "    frequent_terms_in_classes_index.append(frequent_terms_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 สร้างฟังก์ชันในการปรับ prop โดยจะปรับ prop ของ label class ขึ้น 0.2 หาก document มี frequent term ของ label class นั้น ตามเซลล์โค้ดที่ 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 4.3 ###\n",
    "\n",
    "def adjust_label_class_prop(X, prop, label_idx:int):\n",
    "    adjusted_prop = [prop[0], prop[1]]\n",
    "    for frequent_term_idx in frequent_terms_in_classes_index[label_idx]:\n",
    "        \n",
    "        # Check if doc has a frequent term\n",
    "        if (X[frequent_term_idx] > 0):\n",
    "            adjusted_prop[1] += 0.2\n",
    "            break\n",
    "    return adjusted_prop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 สร้างฟังก์ชันในการใช้ทำนายผลลัพธ์จากการปรับ prob ของแต่ละ label class ตามเซลล์โค้ดที่ 4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 4.4 ###\n",
    "\n",
    "def predict_proposed_idea(X):\n",
    "    clf = results1[3]['best_estimator']\n",
    "    pred_prob = clf.predict_proba(X)\n",
    "    \n",
    "    y_pred = list()\n",
    "    for i in range(len(X)):\n",
    "        pred = list()\n",
    "        for label_idx in range(23):\n",
    "            prop = pred_prob[label_idx][i]\n",
    "            adjusted_prop = adjust_label_class_prop(X[i], prop, label_idx)\n",
    "            pred_label_class = 1 if adjusted_prop[1] > adjusted_prop[0] else 0\n",
    "            pred.append(pred_label_class)\n",
    "        y_pred.append(pred)\n",
    "    return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5 ทำการทดสอบโมเดลโดยใช้การปรับ prop ด้วย frequent_terms ตามเซลล์โค้ดที่ 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 0.344974, hamming loss: 0.044107\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 4.5 ###\n",
    "\n",
    "# Train-test split with test size of 33%\n",
    "_, X_test, _, y_test = train_test_split(X_tfidf['attacut_tokenizer']['data'], y,\n",
    "    random_state=50, test_size=.33)\n",
    "\n",
    "y_pred = predict_proposed_idea(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_hamming_loss = hamming_loss(y_test, y_pred)\n",
    "print(f'Testing accuracy: {test_accuracy:.6f}, ' +\n",
    "      f'hamming loss: {test_hamming_loss:.6f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากผลลัพธ์ของเซลล์โค้ดที่ 4.5 พบว่าโมเดลมีค่า accuracy เพิ่มขึ้นจาก 0.336155 เป็น 0.344974 และค่า hamming loss ลดลงจาก 0.044490 เป็น 0.044107"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classification 3\n",
    "\n",
    "นำข้อมูลที่ได้เตรียมไว้จากข้อ 2 (X_tfidf) มาทำการทดลองกับโมเดล OneVsRestClassifier โดยใช้ LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 กำหนด model และช่วงของค่า parameter ที่ต้องการทำ tuning ตามเซลล์โค้ดที่ 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Cell 5.1 ###\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        \"model\": LogisticRegression(solver='lbfgs'),\n",
    "        \"params\": {\n",
    "            \"estimator__C\": np.arange(0.2, 1.1, 0.2),\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 ทำการ parameter tuning หา parameter ที่ให้ค่า accuracy ที่ดีที่สุดด้วย 5-fold cross validation (GridSearchCV) ของคู่ model กับข้อมูลที่ผ่านการเตรียมด้วย tokenizer แบบต่างๆ จากนั้นนำมาทดสอบกับ test set เพื่อประเมินประสิทธิภาพ model ตามเซลล์โค้ดที่ 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs tokenized by newmm_tokenizer\n",
      "- Tuning OVR using LogisticRegression:\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best tuning: acc 0.2760, params {'estimator__C': 1.0}\n",
      "Testing accuracy: 0.2864, hamming loss: 0.0474\n",
      "----------\n",
      "Docs tokenized by attacut_tokenizer\n",
      "- Tuning OVR using LogisticRegression:\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Best tuning: acc 0.2963, params {'estimator__C': 1.0}\n",
      "Testing accuracy: 0.3178, hamming loss: 0.0449\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "### Code Cell 5.2 ###\n",
    "\n",
    "results2 = list()\n",
    "\n",
    "y = raw_data.iloc[:, 2:]\n",
    "\n",
    "for tokenizer_name, X in X_tfidf.items():\n",
    "    print(f'Docs tokenized by {tokenizer_name}')\n",
    "\n",
    "    # Train-test split with test size of 33%\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X['data'], y,\n",
    "        random_state=50, test_size=.33)\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"- Tuning OVR using {type(model['model']).__name__}:\")\n",
    "\n",
    "        # Search for best set of parameters, using 5-fold cross validation\n",
    "        ovr_model = OneVsRestClassifier(model['model'])\n",
    "        clf = GridSearchCV(\n",
    "            ovr_model, model['params'], cv=5, n_jobs=-1, verbose=1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(\n",
    "            f'Best tuning: acc {clf.best_score_:.4f}, params {clf.best_params_}')\n",
    "\n",
    "        # Test model with best set of parameters from GridSearchCV\n",
    "        y_pred = clf.best_estimator_.predict(X_test)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        test_hamming_loss = hamming_loss(y_test, y_pred)\n",
    "        print(f'Testing accuracy: {test_accuracy:.4f}, ' +\n",
    "              f'hamming loss: {test_hamming_loss:.4f}')\n",
    "\n",
    "        result = {\n",
    "            'model': type(model['model']).__name__,\n",
    "            'tokenizer': tokenizer_name,\n",
    "            'best_tuning_params': clf.best_params_,\n",
    "            'best_tuning_acc': clf.best_score_,\n",
    "            'best_estimator': clf.best_estimator_,\n",
    "            'accuracy': test_accuracy,\n",
    "            'hamming_loss': test_hamming_loss}\n",
    "\n",
    "        results2.append(result)\n",
    "\n",
    "    print('-'*10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "จากผลลัพธ์เซลล์โค้ดที่ 5.2 พบว่าผลการทดลองที่ดีที่สุด คือ การเตรียมข้อมูลด้วย attacut_tokenizer และใช้ OneVsRestClassifier กับ LogisticRegression ที่กำหนดค่า C = 1.0 ซึ่งมีค่า accuracy = 0.3178 และ hamming loss = 0.0449"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "จากการทดลองทั้งหมด การจำแนก ticket โดยการเตรียมข้อมูลด้วย attacut_tokenizer และใช้โมเดล RandomForest ให้ผลลัพธ์ที่ดีที่สุด โดยมีค่า accuracy = 0.336155 และ hamming loss = 0.044490 และเมื่อใช้ proposed idea ในการปรับผลลัพธ์ของโมเดล ทำให้โมเดลมีค่า accuracy = 0.344974 (เพิ่มขึ้นมา 0.008819)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fe7d652243bcdb6cde2e69617dc664bf14361ea2de14b139d2fec55ec2ae7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
